%%% Choose between 16:9 and 4:3 format by commenting out/uncommenting one of the following lines:
\documentclass[aspectratio=169]{beamer} % 16:9
% \documentclass{beamer} % 4:3

%=========================================================================================================================

\usepackage[english]{babel}     % English language
\usepackage[latin1]{inputenc}   % Input encoding
\usepackage{tikz}               % For creating graphics
\usepackage[mode=buildnew]{standalone}
\usepackage{url}                % For including urls
\usepackage{tabularx}           % For better tables
\usepackage{xcolor}             % Für bunte Farbe :)

\usetheme{aig}                  % Set beamer theme

%=========================================================================================================================
\title{Qualit\"at von Wikipedia-Artikeln}

% \author[S. Author]{Some Author}
\institute{Artificial Intelligence Group,\\
University of Hagen, Germany}
\date{\today}
%=========================================================================================================================
\logo{\includegraphics[width=3cm]{figures/logoaig.png}}
%=========================================================================================================================

\begin{document}

%=========================================================================================================================

% frame
% block
% alertblock
% exampleblock

% \highlight{}
% \darkhighlight{}
% \yellowhighlight{}
% \mathhighlight{}
% \darkmathhighlight{}
% \yellowmathhighlight{}

% appendix

\begin{frame}
    \titlepage
\end{frame}
\nologo

\section{Problemstellung}

\begin{frame}{Problemstellung}
    \begin{figure}
        \centering
        \includegraphics[width=0.6\linewidth]{figures/example_promo_article.png}
        \caption{\url{https://en.wikipedia.org/wiki/Raison\_(cigarette)}}
        \label{fig:enter-label}
    \end{figure}
\end{frame}

\begin{frame}{Problemstellung}
    \begin{block}{Wann ist ein Wikipedia-Artikel gut?}
        \begin{itemize}
            \item redaktionelle Standards (Good article Kriterien)
            \begin{itemize}
                \item gut geschrieben
            
                \item korrekte und \"uberpr\"ufbare Informationen

                \item neutral in der Sichtweise

                \item verwenden Bilder mit geeigneten Urheberrechtslizenzen
            \end{itemize}

            \item Good article nomination (GAN)

            \item nur 0.59\% der Wikipedia-Artikel sind gut (1 von 171)
        \end{itemize}
    \end{block}

    \begin{block}{Problemstellung}
        Ist ein gegebener Wikipedia-Artikeln gut oder promotional? \\

        Zusatz: Falls ein Wikipedia-Artikel nicht gut ist, warum?
    \end{block}
\end{frame}

\section{Daten}

% 54116 gelabelte Wikipedia Artikel
\begin{frame}{Daten}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/distribution_multiple_classes.png}
    \end{figure}
\end{frame}

\section{Ans\"atze}

\subsection{Logistische Regression}

\begin{frame}{Logistische Regression (1)}
    \begin{block}{Datenvorverarbeitung}
        \begin{itemize}
            \item Entfernung von Sonderzeichen und Kleinbuchstabierung (nativ durch TF-IDF-Vektorisierung)
            \item Stopw\"orter entfernt, aber Zahlen beibehalten
            \item Optimierung der TF-IDF-Vektorisierung durch Gridsearch:
                  \begin{itemize}
                      \item Maximale Anzahl von Features: \{5000, \textbf{10000}, 20000\}
                      \item n-Gramm-Bereich: \{(1,1), (\textbf{1,2}), \dots ,(3,4)\}
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Logistische Regression (2)}
    \begin{block}{Methode}
        \begin{itemize}
            \item Klassifikation mit \texttt{LogisticRegression}:
                  \begin{itemize}
                      \item Solver: \texttt{lbfgs}, \texttt{saga}
                      \item Strafterm: \(L1\) oder \(L2\)-Regularisierung
                      \item Multi-Class: \texttt{OneVsRestClassifier} f\"ur multilabel-Klassifikation
                  \end{itemize}
            \item Optimierung des Modells:
                  \begin{itemize}
                      \item Gridsearch f\"ur Hyperparameter der logistischen Regression:
                            \begin{itemize}
                                \item Regularisierungsparameter \(C\): \{0.01, 0.1, 1, 10, 100\}
                                \item Verschiedene Solver (\texttt{lbfgs}, \texttt{saga})
                            \end{itemize}
                      \item Umgang mit Klassenungleichgewicht: Easy Data Augmentation (EDA) mit nltk.
                            \begin{itemize}
                                \item Synonym Replacement: Es werden zuf\"allig n W\"orter durch Synonyme ersetzt und als neuer Text mit gleichem Label ausgegeben.
                                \item In Pr\"ufung: Random Swap, Random Insertion und Random Deletion
                            \end{itemize}
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Fortschritte durch bisherige Augmentierung}
    \textcolor{red}{Verschieben zu Ausblick?}
    \definecolor{mattegreen}{RGB}{120,180,120} % Mattes Grün
    \definecolor{grey}{RGB}{150,150,150} % Dezentes Grau
    \begin{block}{Verbesserung der Erkennungsraten (Recall) durch Augmentierung}
        \begin{columns}[T,onlytextwidth]
            \column{0.48\textwidth}
            \textbf{Sublabel: advert}
            \begin{itemize}
                \item Original: 0.949
                \item Augmented: 0.900 \tikz[baseline]{\node[anchor=base] {\textcolor{red}{$\searrow$}};}
            \end{itemize}
            
            \textbf{Sublabel: coi}
            \begin{itemize}
                \item Original: 0.000
                \item Augmented: 0.016 \tikz[baseline]{\node[anchor=base] {\textcolor{grey}{$\nearrow$}};}
            \end{itemize}
            
            \textbf{Sublabel: resume}
            \begin{itemize}
                \item Original: 0.408
                \item Augmented: 0.526 \tikz[baseline]{\node[anchor=base] {\textcolor{grey}{$\nearrow$}};}
            \end{itemize}
            
            \column{0.48\textwidth}
            \textbf{Sublabel: fanpov}
            \begin{itemize}
                \item Original: 0.195
                \item Augmented: 0.427 \tikz[baseline]{\node[anchor=base] {\textcolor{mattegreen}{$\uparrow$}};}
            \end{itemize}
            
            \textbf{Sublabel: pr}
            \begin{itemize}
                \item Original: 0.000
                \item Augmented: 0.008 \tikz[baseline]{\node[anchor=base] {\textcolor{grey}{$\nearrow$}};}
            \end{itemize}
        \end{columns}
    \end{block}
\end{frame}

\begin{frame}{Logistische Regression}
    \textcolor{red}{Bild von Ergebnissen}
\end{frame}

\subsection{Support Vector Machine}

\begin{frame}{Support Vector Machine
}
    \begin{block}{Methode}
        \begin{itemize}
            \item TF-IDF-Vektorisierung
            \item Vergleich verschiedener Kernels:
                \begin{itemize}
                    \item \texttt{linear}, \texttt{poly}, \texttt{rbf}, \texttt{sigmoid}.
                    \item Keine Vorteile gegen\"uber \texttt{linear} festzustellen.
                    \item \texttt{LinearSVC} mit deutlich geringerer Laufzeit.
                    \item Parameter-Tuning mit \texttt{GridSearchCV}.
                \end{itemize}
            \item \textbf{Bin\"are Klassifikation}:
                \begin{itemize}
                    \item \texttt{LinearSVC} liefert Precision, Recall, F1-Score und Accuracy jeweils 0.96 - 0.97.
                \end{itemize}
            \item \textbf{Multilabel-Klassifikation}:
                \begin{itemize}
                    \item Klassifikation mit \texttt{MultiOutputClassifier} und \texttt{LinearSVC}.
                    \item Nur \texttt{advert} einigerma\ss{}en identifizierbar mit F1-Score 0.86.
                    \item Bei \texttt{coi}, \texttt{fanpov}, \texttt{pr}, \texttt{resume} F1-Scores von 0.00 bis 0.45.
                    \item Nicht-lineare Kernels liefern \"ahnliche Ergebnisse.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Support Vector Machine}
    \textcolor{red}{Bild von Ergebnissen}
\end{frame}

\subsection{Bayes Klassifikator}

\begin{frame}{Bayes Klassifikator (1)}
    \begin{block}{Datenvorverarbeitung}
        \begin{itemize}
            \item Entfernen von Nicht-Wort-Zeichen
            \item Umwandlung in Kleinbuchstaben
            \item Entfernung von f\"uhrenden und folgenden Leerzeichen
            \item Beibehaltung von Stoppw\"ortern und Zahlen (KPIs nicht beeinflusst)
        \end{itemize}
    \end{block}
\end{frame}
\begin{frame}{Bayes Klassifikator (2)}
    \begin{block}{Methode}
        \begin{itemize}
            \item Textrepr\"asentation mittels TF-IDF-Vektorisierung:
                  \begin{itemize}
                      \item Maximale Anzahl von Features: 10.000
                      \item n-Gramm-Bereich: Unigramme (1,1)
                      \item Minimale Dokumentenfrequenz (\texttt{min\_df}): 0.001
                      \item Maximale Dokumentenfrequenz (\texttt{max\_df}): 0.9
                  \end{itemize}
            \item Einstellung des Gl\"attungsparameters \(\alpha = 1.0\)
            \item \textbf{Bin\"are Klassifikation}:
                  \begin{itemize}
                      \item Einsatz von \texttt{MultinomialNB} f\"ur die Klassifikation zwischen promotional und nicht-promotional Artikeln
                  \end{itemize}
            \item \textbf{Multilabel-Klassifikation}:
                  \begin{itemize}
                      \item Verwendung von \texttt{OneVsRestClassifier} mit \texttt{MultinomialNB} f\"ur die Klassifikation in mehrere Klassen
                      \item Behandlung des Klassenungleichgewichts durch Oversampling mit \texttt{RandomOverSampler}
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}
\begin{frame}{Bayes Klassifikator (3)}
    \begin{block}{Ergebnisse}
        \includegraphics[width=6.8cm]{figures/evaluation_nbb.png} \includegraphics[width=6.8cm]{figures/evaluation_nbm.png}
    \end{block}
\end{frame}

\subsection{Convolutional Neural Network}

\begin{frame}{Convolutional Neural Network}
    \begin{block}{Datenvorverarbeitung}
        \begin{itemize}
            \item Tokenisierung mittels Byte Pair Encoding (cl100k\_base)

            \item Tokens in Zahlen konvertieren
    
            \item Konvertierung in PyTorch Tensor

            \item Tokenembedding
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Convolutional Neural Network}
    \begin{block}{Faltung}
        \textcolor{red}{Bild Architektur}
    \end{block}
\end{frame}

\begin{frame}{Convolutional Neural Network}
    \begin{block}{Training/Optimierung}
        \begin{itemize}
            \item Viele Hyperparameter
            \begin{itemize}
                \item Tokenizer

                \item Embedding Dimension

                \item Anzahl Conv Layers

                \item Maximale L\"ange

                \item Dropout

                \item Filtergr\"o{\ss}en

                \item FFN
            \end{itemize}
            
            \item GridSearch
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Convolutional Neural Network}
    \textcolor{red}{Bild von Ergebnissen}
\end{frame}

\section{Ausblick}
\begin{frame}{Ausblick}
    \begin{block}{Ausblick}
        \begin{itemize}
            \item Hyperparameter-Optimierung

            \item Datensatz erweitern
            \begin{itemize}
                \item Wikipedia-Dump
                
                \item Datenaugmentation
                \begin{itemize}
                    \item W\"orter durch Synonyme ersetzen

                    \item Artikel durch LLMs umschreiben lassen

                    \item Artikel zwischen mehreren Sprachen hin und her \"ubersetzen lassen
                \end{itemize}
                
                \item Transfer Learning (wissenschaftliche Paper)
            \end{itemize}

            \item Vortrainierte Modelle verwenden
        \end{itemize}
    \end{block}
\end{frame}

\end{document}
