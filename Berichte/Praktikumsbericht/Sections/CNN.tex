\subsubsection{Datenvorverarbeitung}
Für die Datenvorverarbeitung der Wikipedia-Artikel wird zunächst ein Byte-Pair-Encoding-Algorithmus (BPE) - genauer der Tokenizer \texttt{cl100k\_base} aus der \texttt{tiktoken}-Bibliothek - angewendet. Dabei werden die Rohtexte in eine Sequenz von numerischen Token umgewandelt, die als Grundlage für die weitere Verarbeitung dienen. Anschließend wird der tokenisierte Text in einen Tensor konvertiert. Um eine einheitliche Eingabelänge zu gewährleisten, werden die Sequenzen auf eine fest definierte maximale Länge normiert: Kürzere Sequenzen werden mit einem speziellen Padding-Token aufgefüllt, während längere Sequenzen abgeschnitten werden. Die resultierenden Daten werden in einem PyTorch Dataset organisiert und mittels eines DataLoaders in Batches aufgeteilt, was eine effiziente Verarbeitung und einen reibungslosen Trainingsablauf ermöglicht.

\subsubsection{Modellarchitektur}
Das eingesetzte Modell basiert auf einem Convolutional Neural Network (CNN). Zunächst wird der tokenisierte Eingabetext über eine Einbettungsschicht (Embedding Layer) geleitet, die die diskreten Token in dichte, kontinuierliche Vektoren umwandelt. Auf den resultierenden Embeddings werden anschließend mehrere 1D-Convolutional Layers mit unterschiedlichen Filtergrößen angewendet. Diese Filter erfassen lokale Muster und n-Gramme im Text, wobei jede Faltungsoperation von einer ReLU-Aktivierungsfunktion gefolgt wird, um nichtlineare Zusammenhänge zu modellieren. Im Anschluss erfolgt ein Global Max-Pooling, das die wichtigsten Merkmale aus den erzeugten Feature-Maps extrahiert. Durch den Einsatz von Dropout wird zudem das Risiko eines Overfittings reduziert, bevor die gewonnenen Merkmale in einem Fully Connected Layer final zu Klassifikationslogits verarbeitet werden.

- Bild von Modellarchitektur? Oder von Faltungsoperation?

\subsubsection{Training}
Für das Training der Modelle kommen unterschiedliche Verlustfunktionen zum Einsatz, abhängig von der spezifischen Aufgabenstellung. Im binären Klassifikationsfall, bei dem entschieden wird, ob ein Artikel neutral oder nicht neutral ist, wird die \texttt{CrossEntropyLoss} verwendet. Im Multilabel-Fall, in dem nicht-neutrale Artikel in mehrere Kategorien wie \textit{fanpov} oder \textit{resume} eingeordnet werden, wird die \texttt{BCEWithLogitsLoss} genutzt, da ein Artikel gleichzeitig mehreren Klassen zugeordnet werden kann. Die Anpassung der Modellparameter erfolgt mithilfe des Adam-Optimierers. Aufgrund des hohen Rechenaufwands und der großen Datenmenge wird das Training auf einer GPU durchgeführt, um die Berechnungen erheblich zu beschleunigen.

\subsubsection{Hyperparameter-Optimierung}
Um die Performance des Modells weiter zu verbessern, wird eine systematische Hyperparameter-Optimierung durchgeführt. Dabei werden Parameter wie die Lernrate, die Anzahl der Filter, die Filtergrößen, die Dropoutrate sowie die maximale Sequenzlänge variiert und optimiert. Mithilfe von Cross-Validation und eines separaten Validierungsdatensatzes wird sichergestellt, dass die gewählten Hyperparameter zu einer guten Generalisierungsfähigkeit des Modells führen.
