Nachdem die ersten vier Ansätze zwar gute Ergebnisse bei der binären Klassifikation gezeigt haben, zeigten sie Schwächen in der Mehrklassenklassifikation. Aus diesem Grund wurden die Daten noch einmal genauer analysiert. Dabei wurden einzelne Artikel noch einmal stichprobenartig gewählt und analysiert. Das Ziel dieser Analyse war es, die Unterschiede der einzelnen Promotional-Klassen zu erkennen. Dabei stellte sich heraus, dass der Hauptunterschied weniger an der Struktur oder Ähnlichem liegt, sondern am Thema des Artikels zu liegen scheint. Aus diesem Grund wurden Recherchen gemacht die zur Transformer Architektur, genauer gesagt BERT führten. Im folgenden werden diese Architekturen etwas genauer vorgestellt.
\paragraph{Transformers}
Die Transformer Architektur wurde zum ersten mal von Vaswani et al. \cite{Vaswani2017} vorgestellt. Als Eingabe des Transformers werden zu den einzelnen Zeichen Positional Encoding dazugegeben und anschließend werden sie durch Embeddings kodiert. Der Transformer selbst wird in einen Encoder und einen Decoder unterteilt wird. Sowohl Encoder als auch Decoder bestanden dort aus 6 Schichten. Im Encoder wurden diese 6 Schichten in eine Multihead-Attention Schicht und ein Vorwärtsgerichtetes Netzwerk unterteilt. Die 6 Schichten des Decoders haben ebenfalls diese 2 Unterschichten. Es kommt hier allerdings noch eine weitere Schicht hinzu, die eine Multihead-Attention über die Ausgabe ausführt. Die Attention-Funktionen werden mithilfe des Skalierten Skalarproduktes berechnet. Diesem werden als Eingabe Abfragen, Schlüssel und Werte übergeben. Diese werden in Form der Matrizen $Q$ für die Abfragen, $V$ für die Werte und $K$ für die Schlüssel übergeben. Das Skalarprodukt berechnet sich dann wie folgt:
$${Attention(Q,K,V)} = {\frac{QK^T}{\sqrt{d_k}} V}$$
Es werden mehrere Attentionfunktionen über Teilmengen der Eingabedaten gleichzeitig berechnet und anschließend konkateniert. Dieses Konzept führt zur Multi-head attention. Die Attention-funktion kommt in drei Punkten zum Vorschein. Die Encoder-Decoder Attention sorgt, dass der Decoder auf alle Ausgaben des Encoders zugreifen kann. Die Encoder Attention sorgt, dass der Encoder auf alle seine vorherigen Schichten zugreifen kann und der Die Decoder Self Attention sorgt, dass der Decoder nur auf seine Vorherigen Schichten zugreifen kann.
\paragraph{Bidirectional Encoder Representations from Transformers}
Der Bidirectional Encoder Representations from Transformers (BERT) wurde von Devlin et al.  \cite{Devlin2018} vorgestellt. BERT wurde so entwickelt, dass es auf einem riesigen nicht-gelabelten Datensatz trainiert wird. Das erlaubt dem Modell mithilfe einer weiteren Schicht auf eine große Anzahl Aufgaben abgestimmt zu sein. Während die ursprüngliche Variante von Transformern nur die vorherigen Wörter in die Berechnung einbezieht, betrachtet BERT auch die Wörter die nach dem Wort stehen. Dadurch bezieht es Kontext von beiden Seiten mit ein. BERT arbeitet, indem es manche Wörter maskiert und versucht diese anhand ihres Kontexts zu bestimmen. BERT wurde zunächst mit unüberwachtem Lernen trainiert und kann anschließend gefine-tuned werden. Eine der Eigenschaften, die dazu geführt haben, für dieses Praktikum BERT zu verwenden, war dass es auf u.a. auf dem Wikipedia Corpus trainiert worden ist. Dadurch kannte es den Aufbau der Artikel bereits und musste nur noch gefine-tuned werden, sodass es Daten klassifizieren kann.

\paragraph{Tokenizer}
%https://arxiv.org/pdf/1609.08144
%https://huggingface.co/learn/nlp-course/chapter6/6
Im Vergleich zu den anderen genannten Ansätzen wurde dieses Modell wie erwähnt aufgrund der Transfer Learning Eigenschaft gewählt. Aus diesem Grund wurde daher auf die TF-IDF Vektorisierung verzichtet und stattdessen wurde der DistilBert Tokenizer verwendet. Das Vokabular des Tokenizers umfasst 30522 Wörter. Es basiert auf Word Piece. Word Piece unterteilt zunächst die Wörter und ergänzt spezialtokens. Im Beispiel von BERT würde sich daraus folgende Darstellung für das Wort token ergeben: \#t \#o \#k \#e \#n
Daraufhin lernt der Tokenizer Mergeregeln. Um die Vereingung von Zeichen durchzuführen verwendet der Tokenizer die folgende Formel:
$$
    \text{score} = \frac{\text{freq\_of\_pair}}{\text{freq\_of\_first\_element} \cdot \text{freq\_of\_second\_element}}
$$
Wordpiece speichert die ermittelteten Wörter anschliessend als Vokabular ab. Zusätzlich gibt es noch Tokens für die speziellen Tokens, die z.B. das Ende eines Textes bezeichnen.

\paragraph{Implementierung}
Der verwendete Modeltyp ist DistilBERT von Sanh et al. \cite{Sanh2019DistilBERTAD}. Der Vorteil dieses Typen ist, dass er die Genauigkeit von BERT weitgehend behält, aber eine deutlich schnellere Laufzeit hat. Obwohl das Modell mit verschiedenen Einstellungen verwendet worden ist, wurden weitgehend die Standardeinstellungen behalten. Das Modell verwendet die Attention des skalierten Skalarproduktes. Es arbeitet mit einer maximalen Eingabelänge von 512, beinhaltet 6 versteckte Schichten im Encoder von denen die Vorwärtsgerichteten Netze jeweils 3072 Neuronen beinhalten und verwendet als Aktivierungsfunktion die GeLu-funktion \cite{hendrycks2023gaussianerrorlinearunits}:
$$GELU(x) = x \cdot P(X \leq x) = x \cdot \Phi(x) $$
Der verwendete Optimizer war AdamW und die Loss-funktion war der Cross-Entropy-loss:
\begin{equation*}
    \mathcal{L} = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
\end{equation*}