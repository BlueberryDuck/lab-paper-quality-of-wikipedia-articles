Nachdem die ersten vier Ansätze gute Ergebnisse bei der binären Klassifikation erzielten, traten Schwächen bei der Multi-Label-Klassifikation auf. Weitere Analysen zur Unterscheidung der einzelnen Promotional Labels ergaben, dass die wesentlichen Unterschiede weniger in der Struktur als vielmehr im thematischen Inhalt der Artikel zu liegen schienen. Daraufhin wurde BERT als fünfter Ansatz gewählt, da es den Kontext beidseitig einbezieht und so den umgebenden Zusammenhang eines Wortes besonders gut erfasst.

BERT basiert auf der Transformerarchitektur, die in einen Kodierer und einen Dekodierer unterteilt ist. Beide bestehen aus $N$ Blöcken. Im Kodierer werden diese in eine Multihead-Attention-Schicht und ein vorwärtsgerichtetes neuronales Netz aufgeteilt. Die Schichten im Dekodierer enthalten neben diesen beiden Unterschichten zusätzlich eine weitere Multihead-Attention, die die Ausgabe des Kodierers verarbeitet. Mithilfe von Attention kann der Transformer seinen Eingaben unterschiedliche Gewichtungen zuweisen. Die Multihead-Attention wird berechnet, indem beispielsweise die Scaled Dot Product Attention \cite{Vaswani2017} mehrfach angewendet und anschließend konkateniert wird. BERT wurde auf einem großen, nicht gelabelten Datensatz vortrainiert und kann durch Transfer Learning, d.h. durch Feintuning mithilfe einer zusätzlichen Schicht \cite{Devlin2018}, für spezifische Anwendungsfälle angepasst werden.

Um Texteingaben zu verarbeiten, kommt im Gegensatz zu den anderen Ansätzen nicht die TF-IDF-Vektorisierung zum Einsatz, sondern der modellinterne Tokenizer. Dieser wurde gewählt, weil technische Unterschiede und die Eigenschaften des Transfer Learnings bei alternativen Kodierungen verloren gingen. Bei der Tokenisierung wird ein Text in kleinere Einheiten, etwa Wörter, zerlegt. Der BERT-Tokenizer, Word Piece, teilt Texte in Teilwörter. Dies erfolgt in zwei Schritten: Zuerst werden Texte in Wörter zerlegt, dann werden diese in Teilwörter unterteilt und mithilfe eines gespeicherten Vokabulars tokenisiert. Das Vokabular entsteht indem Wörter in Buchstaben zerlegt werden. Anschließend werden Spezialzeichen vor jede Einheit gesetzt und die Bestandteile so lange wieder zusammengesetzt, bis ein festgelegtes Wortlimit erreicht ist oder die Zusammengehörigkeitswahrscheinlichkeit zu gering wird. Zusätzlich wird das Vokabular um spezielle Tokens, die beispielsweise den Anfang oder das Ende eines Artikels markieren, ergänzt.

Da die Transformerarchitektur keine rekurrenten Eigenschaften besitzt und somit die Reihenfolge der Sequenzen nicht direkt erfasst, werden den Eingaben Positional Encodings hinzugefügt, die die Position der Tokens in der Sequenz angeben \cite{Song2020} \cite{Sennrich2015} \cite{Schuster2012}.

Im Rahmen des Praktikums kam DistilBERT \cite{Sanh2019} zum Einsatz, da es die Genauigkeit von BERT weitgehend beibehält und gleichzeitig eine deutlich schnellere Laufzeit bietet. Obwohl das Modell mit verschiedenen Einstellungen getestet wurde, wurden final die Standardeinstellungen verwendet. Das Modell nutzt die Scaled Dot Product Attention, arbeitet mit einer maximalen Eingabelänge von 512 Tokens, verfügt über 6 versteckte Schichten im Kodierer, wobei die vorwärtsgerichteten Netze jeweils 3072 Neuronen enthalten, und verwendet die GELU-Aktivierungsfunktion \cite{Hendrycks2016}. Als Verlustfunktion dient die Kreuzentropie, und als Optimierer wird AdamW eingesetzt. Die binäre Klassifikation sowie die Multi-Klassen-Klassifikation erfolgten auf Basis der maximalen Ausgabewahrscheinlichkeit der Neuronen. Im Multilabelfall wurde beim für das Advert-Label zuständigen Neuron eine Sigmoidfunktion angewandt; das Label wurde klassifiziert, wenn der Wert den Schwellwert von 0,5 überschritt. Da die Sigmoidfunktion bei weniger repräsentierten Labels jedoch unzureichende Ergebnisse lieferte, erfolgte die Klassifikation dieser Neuronausgaben mithilfe einer Anomalieerkennung nach Gauß \cite{Mitchell2013}.
