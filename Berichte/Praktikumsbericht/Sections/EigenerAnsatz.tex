Nachdem die ersten vier Ansätze gute Ergebnisse bei der binären Klassifikation gezeigt haben, zeigten sie Schwächen in der Multilabelklassifikation. Nach weiteren Analysen, die das Ziel hatten, die genauen Unterschiede der einzelnen Promotional-Label zu erkennen, stellte sich raus, dass der Hauptunterschied weniger an der Struktur oder Ähnlichem liegt, sondern hauptsächlich am Thema des Artikels zu liegen scheint. Nach weiteren Recherchen wurde der Bidirectional Encoder Representations from Transformers (BERT) als fünfter Ansatz gewählt, weil dieser, den beidseitigen Kontext, das heisst die Wörter links und rechts vom Wort, mit einbezieht und daher besonders gut in der Erkennung von Kontext ist.
\\
BERT ist ein Transformer. Transformer werden in einen Kodierer und einen Dekodierer unterteilt. Kodierer und Dekodierer bestehen jeweils aus $N$ Blöcken. Im Kodierer sind diese in eine Multihead-Attention Schicht und ein Vorwärtsgerichtetes neuronales Netzwerk unterteilt. Die Schichten des Dekodierer haben ebenfalls diese 2 Unterschichten. Allerdings haben sie noch eine weitere Schicht, die eine Multihead-Attention über die Ausgabe des Kodierer ausführt. Durch Attention kann der Transformer über seine Eingaben verwalten und manchen Eingaben entsprechend mehr Beachtung schenken und anderen weniger. Multihead-Attention wird berechnet, indem eine attention, wie z.B. die Scaled Dot Product Attention  \cite{Vaswani2017} mehrmals berechnet und anschliessend konkateniert wird.
(BERT) wurde so entwickelt, dass es auf einem riesigen nicht-gelabelten Datensatz trainiert wird. Das erlaubt dem Modell mithilfe einer weiteren Schicht auf eine spezifische Aufgabe abgestimmt zu werden. Diese Fähigkeit nennt man Transfer Learning \cite{Devlin2018}. \\
Damit der Transformer die Eingaben, die ursprünglich als Text vorlagen verwenden konnte, wurde im Vergleich der anderen Methoden nicht die TF-IDF Vektorisierung, sondern der Modelleigene Tokenizer verwendet. Der Gründe dafür waren zum die technischen Unterschiede der Verfahren und zum Anderen die Eigenschaft des Transfer Learnings, welche z.T. verloren gegangen wäre, wenn man eine andere Kodierung verwendet hätte. Bei der Tokenization wird ein Text in kleinere Einheiten, wie z.B. Wörter unterteilt. Der BERT-Tokenizer, Word Piece, unterteilt die die Texte in Teilwörter. Word Piece wird in zwei Schritte unterteilt. Im ersten Schritt werden die Texte auf Wörter aufgeteilt und im zweiten Schritt werden die Wörter in Teilwörter unterteilt und mithilfe des gespeicherten Vokabulars tokenized. Das Vokabular wird aufgebaut, indem wenn neue Wörter hinzukommen, dass sie in ihre kleinsten Einheiten, z.B. Buchstaben zerlegt werden und anschliessend Spezialzeichen vor jede Einheit eingefügt werden. Anschliessend werden sie nach der Wahrscheinlichkeit, wie sehr sie zusammengehören, zusammengesetzt bis ein vorher bestimmtes Wortlimit erreicht ist oder die Wahrscheinlichkeit, dass diese beiden Buchstaben zusammengehören sollten zu klein ist. Dem ermittelten Vokabular werden ebenfalls Spezialtokens ergänzt, die z.B. den Anfang und das Ende eines eines Artikels signalisieren. \cite{DBLP:journals/corr/abs-2012-15524}, \cite{DBLP:journals/corr/SennrichHB15}, \cite{6289079}

%Weil dieser Ansatz aufgrund des Transfer Learning gewählt worden ist, wurde statt der TF-IDF Vektorisierung der modelleigene Tokenizer verwendet. Er basiert auf dem Word Piece Modell. Der genaue Algorithmus dieses Modells ist nicht Open Source. Laut  \cite{WordpieceSource}, der Seite, von der das Modell und der Tokenizer genommen worden ist, funktioniert Word Piece wahrscheinlich wie folgt: Zunächst unterteilt es die Wörter und ergänzt Spezialtokens. Daraus würde sich folgende Darstellung für das Wort token ergeben: #t #o #k #e #n
%Daraufhin lernt der Tokenizer Mergeregeln, um die Wörter als Tokens wieder zusammenzusetzen. Dabei muss ein Token nicht zwingend einem Wort entsprechen. Nachdem alle Tokens berechnet sind, speichert Word Piece die ermittelten Wörter anschliessend, neben weiteren Spezialtokens als Vokabular ab.

\\
Im Rahmen des Praktikums wurde DistilBERT \cite{Sanh2019} verwendet, welches den Vorteil hat, dass es die Genauigkeit von BERT weitgehend behält, aber eine deutlich schnellere Laufzeit hat. Obwohl das Modell mit verschiedenen Einstellungen getestet worden ist, wurden die Standardeinstellungen weitgehend behalten. Das Modell verwendet die scaled dot product attention. Es arbeitet mit einer maximalen Eingabelänge von 512 Tokens, beinhaltet 6 versteckte Schichten im Kodierer von denen die Vorwärtsgerichteten Netze jeweils 3072 Neuronen beinhalten und verwendet als Aktivierungsfunktion die GELU-Funktion. \cite{Hendrycks2016} Wie beim neuronalen Netz wurde als Loss Funktion die Kreuzenthropie verwendet und als Optimizer AdamW. Während die binäre Klassifizierung und Multiklassenklassifizierung aufgrund der maximalen Ausgabewahrscheinlichkeit klassifiziert hat, wurde im Multilabelfall auf das Ausgabeneuron, dass für die Klassifizierung des Advert Labels verantwortlich war eine Sigmoidfunktion angewendet, wobei das Label klassifiziert worden ist, wenn der Wert einen Schwellwert von 0.5 überschritten hat. Weil die Sigmoid-funktion allerdings schlechte Ergebnisse bei den weniger vertretenen Labels gezeigt hat, wurden die Ausgaben der Neuronen mithilfe von Anomalie Erkennung nach Gauss \cite{GaussMashineLearning} klassifiziert.