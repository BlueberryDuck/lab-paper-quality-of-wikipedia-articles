Nachdem die ersten vier Ansätze gute Ergebnisse bei der binären Klassifikation gezeigt haben, zeigten sie Schwächen in der Multi-Label-Klassifikation. Nach weiteren Analysen, die das Ziel hatten, die genauen Unterschiede der einzelnen Promotional-Klassen zu erkennen, stellte sich raus, dass der Hauptunterschied weniger an der Struktur oder Ähnlichem liegt, sondern am Thema des Artikels zu liegen scheint. Nach weiteren Recherchen wurde der Bidirectional Encoder Representations from Transformers (BERT) als fünfter Ansatz gewählt, weil dieser, den beidseitigen Kontext miteinbezieht und daher besonders gut in der Erkennung von Kontext ist.
\paragraph{Transformers}
BERT ist ein Transformer. Transformer werden in einen Encoder und einen Decoder unterteilt. Encoder und Decoder bestehen jeweils aus $N$ Blöcken. Im Encoder sind diese in eine Multihead-Attention Schicht und ein Vorwärtsgerichtetes Netzwerk unterteilt. Die Schichten des Decoders haben ebenfalls diese 2 Unterschichten. Allerdings haben sie noch eine weitere Schicht, die eine Multihead-Attention über die Ausgabe des Encoders ausführt. Durch Attention kann der Transformer über seine Eingaben verwalten. Der Attention des Skalierten Skalarproduktes werden als Eingabe Abfragen $Q$, Schlüssel $K$ und Werte $V$ übergeben. Das Skalarprodukt berechnet sich dann wie folgt:
$${Attention(Q,K,V)} = {\frac{QK^T}{\sqrt{d_k}} V}$$
Die Attention wird mehrmals über Teilmengen der Eingabedaten berechnet und anschließend konkateniert. Dadurch entsteht Multi-head Attention. \cite{Vaswani2017}
(BERT) wurde so entwickelt, dass es auf einem riesigen nicht-gelabelten Datensatz trainiert wird. Das erlaubt dem Modell mithilfe einer weiteren Schicht auf eine spezifische Aufgabe abgestimmt zu werden. Diese Fähigkeit nennt man Transfer Learning. %BERT wurde zunächst mit unüberwachtem Lernen trainiert und kann anschließend gefine-tuned werden. %Eine der Eigenschaften, die dazu geführt haben, für dieses Praktikum BERT zu verwenden, war dass es auf u.a. auf dem Wikipedia Corpus trainiert worden ist. Dadurch kannte es den Aufbau der Artikel bereits und musste nur noch gefine-tuned werden, sodass es Daten klassifizieren kann. 
\cite{Devlin2018}.

\paragraph{Tokenizer}
%https://arxiv.org/pdf/1609.08144
%https://huggingface.co/learn/nlp-course/chapter6/6
Weil dieser Ansatz aufgrund des Transfer Learning gewählt worden ist, wurde statt der TF-IDF Vektorisierung der DistilBERT Tokenizer verwendet. Er basiert auf Word Piece. Word Piece unterteilt die Wörter und ergänzt spezialtokens. Daraus würde sich folgende Darstellung für das Wort token ergeben: \#t \#o \#k \#e \#n
%Daraufhin lernt der Tokenizer Mergeregeln.
Um die anschliessende Vereingung von Zeichen durchzuführen verwendet der Tokenizer die folgende Formel:
$$
    \text{score} = \frac{\text{freq\_of\_pair}}{\text{freq\_of\_first\_element} \cdot \text{freq\_of\_second\_element}}
$$
Wordpiece speichert die ermittelteten Wörter anschliessend, mit weiteren Spezialtokens als Vokabular ab.

\paragraph{Implementierung}
Das Verwendete Modell war DistilBERT \cite{Sanh2019}, welches den Vorteil hat, dass es die Genauigkeit von BERT weitgehend behält, aber eine deutlich schnellere Laufzeit hat. Obwohl das Modell mit verschiedenen Einstellungen getestet worden ist, wurden die Standardeinstellungen weitgehend behalten. Das Modell verwendet die Attention des skalierten Skalarproduktes. Es arbeitet mit einer maximalen Eingabelänge von 512 Tokens, beinhaltet 6 versteckte Schichten im Encoder von denen die Vorwärtsgerichteten Netze jeweils 3072 Neuronen beinhalten und verwendet als Aktivierungsfunktion die GELU-funktion \cite{Hendrycks2016}:
$$GELU(x) = x \cdot P(X \leq x) = x \cdot \Phi(x) $$
%Der verwendete Optimizer war AdamW und die Loss-funktion war der Cross-Entropy-loss:
%\begin{equation*}
%    \mathcal{L} = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
%\end{equation*}
