\subsubsection{Variante, die beschreibt warum wir welchen Ansatz gewählt haben}
Nachdem die ersten vier Ansätze zwar gute Ergebnisse bei der binären Klassifikation gezeigt haben, zeigten sie Schwächen in der Mehrklassenklassifikation. Aus diesem Grund wurden die Daten noch einmal genauer analysiert. Dabei wurden einzelne Artikel noch einmal stichprobenartig gewählt und analysiert. Das Ziel dieser Analyse war es, die Unterschiede der einzelnen Promotional-Klassen zu erkennen. Dabei stellte sich heraus, dass der Hauptunterschied weniger an der Struktur oder Ähnlichem liegt, sondern am Thema des Artikels zu liegen scheint. Fanpov bezieht sich in diesem Zusammenhang z.B. eher auf Bücher, Serien o.Ä. und PR auf Personen. Durch diese Eigenschaft war wichtig, dass die Bedeutung oder der Kontext der Wörter miteinbezogen wird. Hierbei wurden zwei Ansätze überlegt, wie man die Bedeutung miteinbeziehen kann. Zum einen gab es Gedanken darüber, bereits bei der Kodierung der Daten, die Wörter selbst zu klassifizieren \ref{Ausblick} und die daraus entstandenen Wortkodierungen auf die vorher bereits implementierten Ansätze anzuwenden. Der anderen Gedanke war, das Modell so anzupassen, dass es den Kontext besser verstehen würde. Dieser Gedanke führte zu Recherchen, die zum momentanen State-Of-Art Ansatz mit Transformern führten. Transformer wurden zum ersten mal von Vaswani et al. \cite{Attention} vorgestellt. Die Fähigkeit, die dazu beigetragen hat, dass sie als der letzte Ansatz gewählt worden sind, war das Transfer Learning. Transfer Learning bezeichnet eine Eigenschaft von Large Language Models(LLM).
Diese Eigenschaft bedeutet, dass LLMs auf grossen Textdaten vortrainiert werden und anschliessend auf spezifische Aufgaben wie Textklassifikation fine-tuned werden können \cite{9783960108535}. Das kombiniert mit ihrer Fähigkeit den Kontext von Wörtern zu verstehen, machte diese Art von Modellen zu einer vielversprechenden Lösung für die Klassifizierung von Wikipediaartikeln. Es gibt verschiedene LLMs wie GPT \cite{yenduri2023generativepretrainedtransformercomprehensive}, T5 \cite{DBLP:journals/corr/abs-1910-10683} und BERT \cite{BERTReference}. Laut Zayyanu \cite{article_comparing_LLMs} ist von diesen drei Transformer-Modellen BERT besonders gut, wenn es um das Erkennen des Kontexts von Wörtern geht. Diese Fähigkeit war, wie eben beschrieben, eine wichtige Fähigkeit für die gegebene Problemstellung, weshalb für den 5. Ansatz BERT verwendet und Fine-Tuned worden ist. Das Modell wurde mithilfe von Huggingface \cite{huggingfaceQuelle} geladen und z.T. bearbeitet, um Experimente zu machen, die in \textbf{Referenz ergänzen} genauer beschrieben werden.


\

\subsubsection{Zweite Variante Beschreibung technischer Details}
Nachdem die ersten vier Ansätze zwar gute Ergebnisse bei der binären Klassifikation gezeigt haben, zeigten sie Schwächen in der Mehrklassenklassifikation. Aus diesem Grund wurden die Daten noch einmal genauer analysiert. Dabei wurden einzelne Artikel noch einmal stichprobenartig gewählt und analysiert. Das Ziel dieser Analyse war es, die Unterschiede der einzelnen Promotional-Klassen zu erkennen. Dabei stellte sich heraus, dass der Hauptunterschied weniger an der Struktur oder Ähnlichem liegt, sondern am Thema des Artikels zu liegen scheint. Aus diesem Grund wurden recherchen gemacht die zur Transformer Architektur, genauer gesagt BERT führten. Im folgenden werden diese Architekturen etwas genauer vorgestellt.
\paragraph{Transformers}
Die Transformer Architektur wurde zum ersten mal von Vaswani et al. \cite{Attention} vorgestellt. Als Eingabe des Transformers werden zu den einzelnen Zeichen Positional Encoding dazugegeben und anschliessend werden sie durch Embeddings kodiert. Der Transformer selbst wird in einen Encoder und einen Decoder unterteilt wird. Sowohl Encoder als auch Decoder bestanden dort aus 6 Schichten. Im Encoder wurden diese 6 Schichten in eine Multihead-Attention Schicht und ein Vorwärtsgerichtetes Netzwerk unterteilt. Die 6 Schichten des Decoders haben ebenfalls diese 2 Unterschichten. Es kommt hier allerdings noch eine weitere Schicht hinzu, die eine Multihead-Attention über die Ausgabe ausführt. Die Attention-Funktionen werden mithilfe des Skalierten Skalarproduktes berechnet. Diesem werden als Eingabe Abrfragen, Schlüssel und Werte übergeben. Diese werden in Form der Matrizen $Q$ für die Abfragen, $V$ für die Werte und $K$ für die Schlüssel übergeben. Das Skalarprodukt berechnet sich dann wie folgt:
$${Attention(Q,K,V)} = {{{QK^T} \over \sqrt{d_k}} V}$$
Es werden mehrere Attentionfunktionen über Teilmengen der Eingabedaten gleichzeitig berechnet und anschliessend Concateniert. Dieses Konzept führt zur Multi-head attention. Die Attention-funktion kommt in drei Punkten zum Vorschein. Die Encoder-Decoder Attention sorgt, dass der Decoder auf alle Ausgaben des Encoders zugreifen kann. Die Encoder Attention sorgt, dass der Encoder auf alle seine vorherigen Schichten zugreifen kann und der Die Decoder Self Attention sorgt, dass der Decoder nur auf seine Vorherigen Schichten zugreifen kann. 
\paragraph{Bidirectional Encoder Representations from Transformers}
Der Bidirectional Encoder Representations from Transformers (BERT) wurde von Devlin et al.  \cite{BERTReference} vorgestellt. BERT wurde so entwickelt, dass es auf einem riesigen nicht-gelabelten Datensatz trainiert wird. Das erlaubt dem Modell mithilfe einer weiteren Schicht auf eine grosse Anzahl Aufgaben abgestimmt zu sein. Während die ursprüngliche Variante von Transformern nur die vorherigen Wörter in die Berechnung einbezieht, betrachtet BERT auch die Wörter die nach dem Wort stehen. Dadurch bezieht es Kontext von beiden Seiten mit ein. BERT arbeitet, indem es manche Wörter maskiert und versucht diese anhand ihres Kontexts zu bestimmen. BERT wurde zunächst mit unüberwachtem Lernen trainiert und kann anschliessend gefine-tuned werden. Eine der Eigenschaften, die dazu geführt haben, für dieses Praktikum BERT zu verwenden, war dass es auf u.a. auf dem Wikipedia Corpus trainiert worden ist. Dadurch "kennt" es den Aufbau der Artikel bereits und musste nur noch gefine-tuned werden, sodass es Daten klassifizieren kann.