Nachdem die ersten vier Ansätze zwar gute Ergebnisse bei der binären Klassifikation gezeigt haben, zeigten sie Schwächen in der Mehrklassenklassifikation. Aus diesem Grund wurden die Daten noch einmal genauer analysiert. Dabei wurden einzelne Artikel noch einmal stichprobenartig gewählt und analysiert. Das Ziel dieser Analyse war es, die Unterschiede der einzelnen Promotional-Klassen zu erkennen. Dabei stellte sich heraus, dass der Hauptunterschied weniger an der Struktur oder Ähnlichem liegt, sondern am Thema des Artikels zu liegen scheint. Aus diesem Grund wurden recherchen gemacht die zur Transformer Architektur, genauer gesagt BERT führten. Im folgenden werden diese Architekturen etwas genauer vorgestellt.
\paragraph{Transformers}
Die Transformer Architektur wurde zum ersten mal von Vaswani et al. \cite{Attention} vorgestellt. Als Eingabe des Transformers werden zu den einzelnen Zeichen Positional Encoding dazugegeben und anschliessend werden sie durch Embeddings kodiert. Der Transformer selbst wird in einen Encoder und einen Decoder unterteilt wird. Sowohl Encoder als auch Decoder bestanden dort aus 6 Schichten. Im Encoder wurden diese 6 Schichten in eine Multihead-Attention Schicht und ein Vorwärtsgerichtetes Netzwerk unterteilt. Die 6 Schichten des Decoders haben ebenfalls diese 2 Unterschichten. Es kommt hier allerdings noch eine weitere Schicht hinzu, die eine Multihead-Attention über die Ausgabe ausführt. Die Attention-Funktionen werden mithilfe des Skalierten Skalarproduktes berechnet. Diesem werden als Eingabe Abrfragen, Schlüssel und Werte übergeben. Diese werden in Form der Matrizen $Q$ für die Abfragen, $V$ für die Werte und $K$ für die Schlüssel übergeben. Das Skalarprodukt berechnet sich dann wie folgt:
$${Attention(Q,K,V)} = {\frac{QK^T}{\sqrt{d_k}} V}$$
Es werden mehrere Attentionfunktionen über Teilmengen der Eingabedaten gleichzeitig berechnet und anschliessend Concateniert. Dieses Konzept führt zur Multi-head attention. Die Attention-funktion kommt in drei Punkten zum Vorschein. Die Encoder-Decoder Attention sorgt, dass der Decoder auf alle Ausgaben des Encoders zugreifen kann. Die Encoder Attention sorgt, dass der Encoder auf alle seine vorherigen Schichten zugreifen kann und der Die Decoder Self Attention sorgt, dass der Decoder nur auf seine Vorherigen Schichten zugreifen kann.
\paragraph{Bidirectional Encoder Representations from Transformers}
Der Bidirectional Encoder Representations from Transformers (BERT) wurde von Devlin et al.  \cite{BERTReference} vorgestellt. BERT wurde so entwickelt, dass es auf einem riesigen nicht-gelabelten Datensatz trainiert wird. Das erlaubt dem Modell mithilfe einer weiteren Schicht auf eine grosse Anzahl Aufgaben abgestimmt zu sein. Während die ursprüngliche Variante von Transformern nur die vorherigen Wörter in die Berechnung einbezieht, betrachtet BERT auch die Wörter die nach dem Wort stehen. Dadurch bezieht es Kontext von beiden Seiten mit ein. BERT arbeitet, indem es manche Wörter maskiert und versucht diese anhand ihres Kontexts zu bestimmen. BERT wurde zunächst mit unüberwachtem Lernen trainiert und kann anschliessend gefine-tuned werden. Eine der Eigenschaften, die dazu geführt haben, für dieses Praktikum BERT zu verwenden, war dass es auf u.a. auf dem Wikipedia Corpus trainiert worden ist. Dadurch kannte es den Aufbau der Artikel bereits und musste nur noch gefine-tuned werden, sodass es Daten klassifizieren kann.