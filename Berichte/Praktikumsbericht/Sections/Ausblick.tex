\section{Ausblick}
\label{Ausblick}
Nachdem alle Experimente durchgeführt und evaluiert worden sind, werden die gesammelten Erfahrungen festgehalten.

\subsection{Erfolglose Versuche}
Um gute Modelle zu finden, die die Aufgabe lösen konnten, gab es einige Versuche, die zwar vielversprechend schienen, aber in der Praxis keine erfolgreichen Ergebnisse lieferten.

\subsubsection{Gram-Schmidt Verfahren zum Imitieren von Embeddings}
Embeddings wurden in \textbf{Referenz ergänzen} vorgestellt. Ein Problem, die Embeddings haben, ist dass sie vortrainiert werden müssen. Ein Gedanke, der bereits früh in der Entwicklung der Modelle auftrat, war es Embeddings zu imitieren und sie so zu erschaffen, dass es nicht notwendig ist, diese zu trainieren. Yang et al. \cite{Yang2019} hatte eine ähnliche Idee. Die Grundidee ist es, die Sätze aufzuspannen und aufgrund dessen die Ähnlichkeit zu einander zu bestimmen. Im Rahmen des Projektpraktikums wurden daher $n$ Artikel von jeder der $m$ Kategorien verwendet, um eine Orthonormalbasis für einen $m \cdot n$-dimensionalen Raum zu schaffen. Daraufhin wurden die Skalarprodukte zwischen den restlichen Vektoren und jedem Vektor der Orthogonalbasis bestimmt. Anschließend wurde das Skalarprodukt der Vektoren der nicht aufgespannten Vektoren mit dem der orthonormierten Vektoren gebildet. Diese Skalarprodukte, waren jeweils der $x_i$ Koordinatenpunkt, wobei $i$ die Koordinate ist, die durch $n$ aufgespannt wird. Diese wurden anschließend als Eingabe der anderen Modelle verwendet. Das Problem an dieser Lösung war, dass die Resultate schlechter geworden sind, weswegen diese Lösung nicht weiter verfolgt worden ist.

\subsubsection{Klassifizierung einzelner Wörter als Kodierung}
Da die vier ersten verwendeten Ansätze zu Problemen führten, was die Multiklassenklassifikation angeht und daher der Gedanke aufkam den Kontext von Wörtern miteinzubeziehen, gab es den Gedanken, bereits einzelne Wörter zu klassifizieren und anschließend so zu kodieren, dass ähnlich klassifizierte Wörter durch naheliegende Werte dargestellt werden. Dabei wurde das Alphabet als Lexikon verwendet und die Wörter mithilfe dieses Lexikons kodiert. Diese Kodierung wurde dann klassifiziert. Das Problem an diesem Versuch, war es dass bereits die Klassifizierung der einzelnen Wörter zu schlechten Resultaten führen geführt hat, weswegen dieser Versuch an dieser Stelle nicht weiter verfolgt worden ist. Das Verfahren funktioniert grundsätzlich wie folgt: Zunächst werden alle Artikel in ihre Worte zerteilt. Jedes der Wörter wird aufgrund seiner Angehörigkeit gelabelt. Anschließend wird geschaut, ob ein Wort mehrere Label hat. Dieses wird entsprechend separat gelabelt. Bisher wurden nur die Wörter aus den Trainingsdaten gelabelt. Diese Wörter werden in ihre Buchstaben zerlegt und faktorisiert. Anschließend werden sie verwendet, um ein Modell zu trainieren. Neue Wörter, das heißt z.B. Wörter aus den Testdaten, die nicht in den Trainingsdaten waren, werden in einzelne Buchstaben zerlegt und vektorisiert. Daraufhin werden sie klassifiziert und erhalten ihren Code, mit dem sie als Vektor dargestellt werden.

\subsection{Ausbaumöglichkeiten}
Um die Forschung auf diesem Gebiet fortzuführen, könnte man die Modelle auf dem gesamten Wikipedia Dump trainieren. Ausserdem könnte man die Arbeit ausbauen, indem man weitere Modelle hinzufügt. Besonders interessant könnten dabei parameterlose Modelle wie Entscheidungsbäume sein, um die Multilabelklassifikation weiter zu verbessern. Eine weitere Architektur, die positive Effekte zur Bekämpfung der ungleich verteilten Label haben könnte, wäre SetFit \cite{Tunstall2022}. Im Rahmen des Praktikums, lag der Fokus eher auf dem Text der einzelnen Artikel, während die URL nicht zur anwendung kam. Daher könnte man versuchen, in einem weiteren Schritt eher auf dieses Attribut zu fokussieren und die Metadaten der einzelnen Seiten zur Klassifizierung zu verwenden.
