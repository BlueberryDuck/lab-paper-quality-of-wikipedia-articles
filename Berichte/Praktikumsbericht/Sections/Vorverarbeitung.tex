\subsection{Vorverarbeitung und Repräsentation der Daten}
\label{sec:vorverarbeitung}

Die Datenvorverarbeitung umfasst mehrere parametrisch aktivierbare Schritte, um den Einfluss einzelner Bereinigungsschritte auf die Modellleistung evaluieren zu können. Zu den implementierten Verfahren zählen die Entfernung unerwünschter Zeichen, wie Sonder- und Interpunktionszeichen, sowie die Umwandlung aller Zeichen in Kleinbuchstaben. Zusätzlich können häufig vorkommende, inhaltlich wenig relevante Stoppwörter entfernt und optional die Wörter mittels Stemming auf ihre Wortstämme reduziert werden. Überflüssige Leerzeichen und Absatzumbrüche werden entfernt, welche als Artefakte der vorherigen Bereinigungsschritte entstehen können. Es wurde siche gegen das Entfernen von Zahlen entschieden, da diese einen Einfluss auf die Qualität der Wikipedia-Artikel haben können.

Da Machine-Learning-Modelle numerische Eingaben erfordern, wird der bereinigte Text mittels Vektorisierung in numerische Repräsentationen überführt. Zur Umsetzung dieses Schritts wurden verschiedene Verfahren implementiert: Der Count-Vektorisierer beziehungsweise der Bag-of-Words-Ansatz, die die absoluten Wortvorkommen zählen, der TF-IDF-Vektorisierer, der die Häufigkeit eines Wortes in einem Dokument in Relation zu seiner Verbreitung im gesamten Korpus gewichtet. Word2Vec erstellt kontinuierliche Vektoren, bei denen semantisch ähnliche Wörter nahe beieinander liegen, und GloVe nutzt globale Wortkookkurrenzstatistiken, um semantische Beziehungen im Vektorraum abzubilden. Diese vielfältigen Ansätze ermöglichen es, die Textdaten in numerische Merkmalsvektoren zu transformieren, die als Eingabe für die Machine-Learning-Modelle dienen. Final wurde sich für den TF-IDF-Vektorisierer entschieden, da dieser bei stichprobenartiger Evaluation auf den klassischen machinellen Lernverfahren bei kurzer Rechenzeit die besten Ergebnisse lieferte.

% SB: Quellen folgen
