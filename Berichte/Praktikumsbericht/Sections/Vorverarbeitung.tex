\subsection{Vorverarbeitung und Repräsentation der Daten}
\label{sec:vorverarbeitung}

Die Textdaten wurden einer parametrisch aktivierbaren Vorverarbeitung unterzogen, um den Einfluss einzelner Bereinigungsschritte auf die Modellleistung evaluieren zu können. Dabei wurden Sonder- und Interpunktionszeichen entfernt, alle Zeichen in Kleinbuchstaben umgewandelt und häufige Stoppwörter eliminiert. Außerdem wurde der Porter-Stemmer \cite{Porter2006} zur Reduktion auf Wortstämme verwendet, während Zahlen beibehalten wurden, um relevante Informationen nicht zu verlieren.

Für die Umwandlung der bereinigten Texte in numerische Repräsentationen wurden verschiedene Vektorisierungsmethoden in Betracht gezogen. Neben der klassischen Count-Vektorisierung beziehungsweise dem Bag-of-Words-Ansatz kamen auch semantische Ansätze wie Word2Vec \cite{Mikolov2013} und GloVe \cite{Pennington2014} zum Einsatz; final fiel die Wahl auf die TF-IDF-Vektorisierung.

Die TF-IDF-Vektorisierung einer Dokumentensammlung $D = \{d_1, \dots, d_N\}$ mit Vokabular $V = \{t_1, \dots, t_m\}$ wird dabei durch die Matrix $X = (x_{ij})$ abgebildet, wobei $x_{ij} = \mathrm{tf}(t_i,d_j) \cdot \log\frac{N}{df(t_i)}$ gilt. $\mathrm{tf}(t_i,d_j)$ bezeichnet die Termfrequenz, $df(t_i)$ die Dokumentfrequenz des Terms und $\log\frac{N}{df(t_i)}$ die inverse Dokumentfrequenz. Diese Methode reduziert den Einfluss häufiger, uninformativer Wörter, hebt diskriminierende Begriffe hervor und hat sich insbesondere in Anwendungen wie der Textklassifikation und im Information Retrieval zur Merkmalsextraktion bewährt \cite{Manning2009}.

Die praktische Umsetzung erfolgte über den \texttt{TfidfVectorizer} von Scikit-Learn \cite{Pedregosa2011}. Final wurde mit folgenden Parametern gearbeitet: \texttt{ngram\_range: [1, 1]} (ausschließlich Einzelwörter), \texttt{max\_df: 0.9} (Ignorieren von Wörtern, die in über 90\% des Korpus vorkommen), \texttt{min\_df: 0.001} (Entfernung sehr seltener Wörter, mögliche Rechtschreibfehler), \texttt{max\_features: 10.000} (Begrenzung der Vokabulargröße zur Laufzeitverbesserung und Minimierung des Overfit-Risikos) sowie \texttt{sublinear\_tf: true} (logarithmische Skalierung der Termfrequenzen zur Abschwächung extremer Häufigkeiten).
