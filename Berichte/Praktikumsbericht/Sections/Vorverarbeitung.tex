\subsection{Vorverarbeitung und Repräsentation der Daten}
\label{sec:vorverarbeitung}

Die Datenvorverarbeitung umfasst mehrere parametrisch aktivierbare Schritte, um den Einfluss einzelner Bereinigungsschritte auf die Modellleistung evaluieren zu können. Ziel ist es, den Text in eine einheitliche und bereinigte Form zu bringen. Zu den implementierten Verfahren zählen die Entfernung unerwünschter Zeichen, wie Sonder- und Interpunktionszeichen, sowie die Umwandlung aller Zeichen in Kleinbuchstaben - wobei hier die Sorge bestand, semantische Unterschiede zu verlieren (etwa zwischen us, dt. uns, und der Abkürzung US für United States). Zusätzlich können häufig vorkommende, inhaltlich wenig relevante Stoppwörter entfernt und optional die Wörter mittels Stemming auf ihre Wortstämme reduziert werden. Auch überflüssige Leerzeichen und Absatzumbrüche werden entfernt, um den Text auf seine wesentlichen lexikalischen Elemente zu reduzieren und die Rechenzeit zu minimieren.

Da Machine-Learning-Modelle numerische Eingaben erfordern, wird der bereinigte Text mittels Vektorisierung in numerische Repräsentationen überführt. Die Vektorisierung spielt hierbei eine zentrale Rolle, da sie es den Algorithmen ermöglicht, Muster und Zusammenhänge im Text zu erkennen. Zur Umsetzung dieses Schritts wurden verschiedene Verfahren implementiert: Der TF-IDF-Vektorisierer gewichtet die Häufigkeit eines Wortes in einem Dokument in Relation zu seiner Verbreitung im gesamten Korpus, während der Count-Vektorisierer beziehungsweise der Bag-of-Words-Ansatz die absoluten Wortvorkommen zählt. Darüber hinaus werden dichte Wort-Embeddings eingesetzt: Word2Vec erstellt kontinuierliche Vektoren, bei denen semantisch ähnliche Wörter nahe beieinander liegen, und GloVe nutzt globale Wortkookkurrenzstatistiken, um semantische Beziehungen im Vektorraum abzubilden. Diese vielfältigen Ansätze ermöglichen es, die Textdaten in numerische Merkmalsvektoren zu transformieren, die als Eingabe für die Machine-Learning-Modelle dienen.

% SB: Quellen folgen
