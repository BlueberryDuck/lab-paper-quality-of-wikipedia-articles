\subsection{Logistische Regression}

Für die Klassifikation von Labeln wurde die logistische Regression ausgewählt, weil dies insbesondere schnell und einfach zu implementieren war. Um Overfitting zu verhindern, wurden Lasso und Ridge Regularisierung eingesetzt. Außerdem wurden zwei Solver evaluiert: Der \texttt{liblinear}-Solver wurde als Standardmethode eingesetzt, da er sowohl \texttt{L1}- als auch \texttt{L2}-Regularisierung unterstützt. Zusätzlich wurde \texttt{saga}, eine Erweiterung des Stochastic Average Gradient, getestet, die speziell für große Datensätze geeignet ist und ebenfalls mit beiden Regularisierungen kompatibel ist.

Für die Klassifizierung bei der logistischen Regression wird das Optimierungsproblem $\min_\theta L^{LR}(D,h^{LR}_\theta)$ gelöst. Dabei ist D der gegebene Datensatz und $\theta$ die Parameter, über die optimiert wird. Außerdem ist $h_\theta^{LR}$ als Sigmoidfunktion definiert:
\begin{equation*}
    h_\theta^{LR}(x) = \frac{1}{1 + e^{-(\theta_0 + \theta_1 x_1 + \dots + \theta_n x_n)}}
\end{equation*}
L ist die Kostenfunktion und definiert als:
\begin{equation*}
    L_\lambda^{\text{LR}}(D, h^{LR}_\theta)=L^{LR}(D,h^{LR}_\theta)+\lambda R(h^{LR}_\theta) 
\end{equation*}
Mit
\begin{equation*}
     L^{LR}(D,h^{LR}_\theta)=- \sum_{i=1}^{m}  y^{(i)} \ln h^{LR}_\theta(x^{(i)}) + (1 - y^{(i)}) \ln (1 - h^{LR}_\theta(x^{(i)}))
\end{equation*}
und den Regularisierungsmethoden $R(h^{LR}_\theta)=\sum_{i=1}^m\theta^2$ (Ridge-Regression) und $ R(h^{LR}_\theta)=\sum_{i=1}^m\|\theta\|$ (Lasso-Regression).