\subsection{Datenaugmentierung}

\subsection{Transfer Learning und Vortrainierte Embeddings}

https://arxiv.org/pdf/1301.3781 Falls wir Word2Vec verwenden.

https://nlp.stanford.edu/pubs/glove.pdf falls wir GloVe verwenden.


https://arxiv.org/pdf/1607.04606 falls wir Fast Text verwenden.

https://arxiv.org/pdf/1802.05365 Falls wir Elmo verwenden wollen. Ich finde das sehr interessant.


https://arxiv.org/pdf/1810.04805 Falls wir Bert verwenden wollen. Berts Vorteile liegen auch stark darin dass es genau auf dem Wikipedia Corpus trainiert worden ist.

