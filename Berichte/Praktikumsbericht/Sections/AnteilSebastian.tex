\subsubsection{Projektstruktur}
Zu Beginn des Projekts arbeiteten alle Teilnehmenden explorativ in Jupyter Notebooks, um ein grundlegendes Verständnis der Daten zu entwickeln und erste Modellierungsansätze zu testen. Um konsistente Datenverarbeitung und vergleichbare Ergebnisse zu gewährleisten, wurde eine einheitliche Machine-Learning-Pipeline entwickelt.

Diese Pipeline standardisiert die wesentlichen Schritte - von der Datenverarbeitung über die Modellierung bis hin zur Evaluation - und besteht aus den folgenden modularen Komponenten:

\begin{itemize}
    \item \textbf{DataLoader} - Laden und Zusammenführen der Daten
    \item \textbf{Preprocessing} - Vorverarbeitung der Textdaten (z.B. Entfernen von Stopwörtern)
    \item \textbf{Feature Extraction} - Extraktion von Merkmalen (z.B. mittels TF-IDF oder GloVe-Vectorizer)
    \item \textbf{Model} - Training und Hyperparameter-Optimierung
    \item \textbf{Evaluator} - Auswertung der Modellergebnisse
\end{itemize}

Diese Komponenten sind in einer Python-Bibliothek (\texttt{src/}) implementiert, sodass sie sowohl eigenständig als auch in Jupyter Notebooks eingesetzt werden können. Die Pipeline wird über YAML-Konfigurationsdateien gesteuert, die alle relevanten Einstellungen wie Dateipfade, Vorverarbeitungsschritte, Modellparameter und Evaluationskriterien beinhalten. Dadurch ist eine flexible Anpassung an unterschiedliche Use-Cases möglich. Zudem können einzelne Pipeline-Schritte partiell ausgeführt werden - etwa kann die Feature Extraction aus vorherigen Läufen geladen werden, um ausschließlich das Modell mit variierenden Parametern zu trainieren.

Die einheitliche Struktur erleichtert die Weiterentwicklung und Wartung des Codes, da alle Pipeline-Schritte klar getrennt sind. Für detaillierte Implementierungsinformationen und Konfigurationsdetails wird auf das GitHub-Repository verwiesen.

\subsubsection{Vectorizer}
Um die Textdaten in ein für die Modelle verarbeitbares Format zu bringen, wurden verschiedene Vectorizer implementiert. Diese umfassen:

\begin{itemize}
    \item \textbf{TF-IDF Vectorizer} - Verwendet den TF-IDF-Ansatz, um die Wichtigkeit von Wörtern in Dokumenten zu gewichten.
    \item \textbf{Count Vectorizer} - Zählt die Häufigkeit von Wörtern in den Textdaten.
    \item \textbf{Bag of Words Vectorizer} - Eine spezielle Form des Count Vectorizers, die binäre Werte verwendet, um die Präsenz von Wörtern anzuzeigen.
    \item \textbf{Word2Vec Vectorizer} - Nutzt das Word2Vec-Modell, um Wörter in kontinuierliche Vektoren zu transformieren.
    \item \textbf{GloVe Vectorizer} - Verwendet vortrainierte GloVe-Modelle, um Wörter in Vektoren zu transformieren.
\end{itemize}

Diese Vectorizer sind in der Python-Bibliothek (\texttt{src/vectorizer/}) implementiert und können flexibel in der Pipeline eingesetzt werden, um die besten Ergebnisse für verschiedene Anwendungsfälle zu erzielen.
