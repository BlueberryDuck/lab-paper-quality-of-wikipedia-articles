
Bevor die Modellbildung starten konnte, mussten zunächst die Bedingungen geklärt werden. Das bedeutet, dass der Datensatz analysiert werden musste und eine entsprechende Problemstellung identifiziert werden musste. Im Folgenden wird zunächst der ursprüngliche Datensatz beschrieben und anschliessend erklärt, wie dieser in Laufe des Projekts ausgebaut und ergänzt wird. Danach wird die daraus abgeleitete Problemstellung erläutert.

\subsection{Ursprünglicher Datensatz}
%Erter Satz geändert
Als Basis wurde ein Datensatz verwendet, welcher  verschiedene Wikipedia-Artikel enthält \cite{UrsprungDatensatz}. Der Datensatz wurde in zwei Dateien unterteilt. Die erste Datei enthielt \emph{promotional} (also werbend) Artikel und wurde noch weiter unterteilt. Dabei sind folgende Label vergeben:
\begin{itemize}
    \item advert – „Dieser Artikel enthält Inhalte, die wie eine Werbeanzeige verfasst sind.“
\item coi – „Ein Hauptautor dieses Artikels scheint eine enge Verbindung zu seinem Thema zu haben.“
\item fanpov – „Dieser Artikel ist möglicherweise aus der Sicht eines Fans geschrieben, statt aus einer neutralen Perspektive.“
\item pr – „Dieser Artikel liest sich wie eine Pressemitteilung oder ein Nachrichtenartikel oder basiert weitgehend auf routinemäßiger Berichterstattung oder Sensationslust.“
\item resume – „Dieser biografische Artikel ist wie ein Lebenslauf geschrieben.“
\end{itemize}
Der zweite Datensatz enthält Wikipedia Artikel die \emph{nicht-promotional} klassifiziert sind.
\\ \\
Der zweite Datensatz enthielt Artikel, die als Good klassifiziert worden sind. Die beiden Datensätze wurden zusammen verwendet, um die gesamte Datenbasis zu bilden. Insgesamt ergab das \textbf{Anzahl Datensätze einfügen} Daten. Die Label dieser Daten war wie folgt verteilt:


\textbf{2 Bilder einfügen: 1. Bild einzelne Label, 2. Bild Kombination der Label}
\\



\subsection{Probleme des ursprünglichen Datensatzes}
Wie man anhand des Diagramms \textbf{Auf Bild referenzieren} sehen kann, sind die Daten ungleich verteilt. Daten mit dem Label \textit{good} nehmen laut \textbf{Quelle einfügen} nur 0.59\% aller Wikipediaartikel ein. Allerdings sieht man anhand der Grafik, dass sie im Vergleich zu \textbf{Prozentsatz berechnen}. Zum einen führt das zu einem Verhältnis, das nicht der Realität entspricht, zum anderen kann das die Ergebnisse der trainierten Modelle verschlechtern.



\subsection{Weitere Daten}
Um ein gutes Modell zu erstellen, welches auf Maschinellem Lernen basiert, braucht man entsprechende Datensätze. Wie in Abschnitt \textbf{Referenz einfügen} besprochen worden ist, ist der ursprüngliche Datensatz nicht ausreichend, um entsprechende Modelle zu trainieren. Aus diesem Grund wurden verschiedene Methoden ausprobiert und verwendet, um den ursprünglichen Datensatz zu erweitern.

\subsubsection{Datensatzerweiterung durch Wikipedia-Dump}
\label{WPDump}
Die naheliegendste Methode zur Erweiterung eines Datensatzes, ist das Hinzuziehen neuer Daten. Hierzu diente der Wikipedia-Dump, ein offizieller Datensatz, der alle Wikipedia-Seiten umfasst. Er ist für die verschiedenen Wikipedia-Sprachversionen und in Varianten mit oder ohne Historie verfügbar. Hier wurde der Dump der englischsprachigen Wikipedia ohne Historie verwendet, der unter \url{https://dumps.wikimedia.org/enwiki/20241220/} zu finden ist. Er besteht aus zwei Dateien:

\begin{itemize}
    \item \emph{enwiki-20241020-pages-articles-multistream.xml.bz2} (circa 22 GB komprimiert und 97 GB entpackt): Enthält eine komprimierte XML-Datei mit allen 24.091.931 Seiten und den dazugehörigen Metadaten.
    \item \emph{enwiki-20241020-pages-articles-multistream-index.txt.bz2} (circa 250 MB komprimiert und 1 GB entpackt): Enthält eine Index-Datei mit 240.953 Offsets in der komprimierten XML-Datei zwischen denen jeweils 100 Seiten liegen.
\end{itemize}

Aufgrund der Größe des Dumps konnte dieser nicht komplett in den Arbeitsspeicher geladen werden. Unter Nutzung der in der Index-Datei gespeicherten Offsets wurden daher einzelne Abschnitte von jeweils 100 Seiten entpackt und verarbeitet.

Da die Seiten im Dump in Wiki-Sytax vorliegen, können die von den Wikipedia-Autoren als lesenswert (\emph{good}), exzellent (\emph{featured}) oder werbend (\emph{promotional}) kategorisierten Artikel anhand der zugehörigen Vorlagen (Templates) identifiziert werden, mit denen die Artikel für den Leser entsprechend gekennzeichnet werden. Für \emph{good} und \emph{featured} ist dabei jeweils nur ein Template vorgesehen, dass in Wiki-Syntax als \textit{\{\{good article\}\}} beziehungsweise \textit{\{\{featured article\}\}} im Quellcode auftaucht, wobei abweichende Groß- und Kleinschreibung möglich ist. Für \emph{promotional} sind hingegen 21 verschiedene Templates möglich, die unterschiedliche Arten von werbendem Inhalt kennzeichnen. Außerdem lassen sich anhand von Templates und anderen Syntax-Elementen noch diejenigen Seiten identifizieren, welche keine Artikel darstellen, darunter Begriffsklärungsseiten, Umleitungen, Kategorien, Benutzerseiten und weitere Namensräume.

Mithilfe eines Python-Programms wurden alle Artikel aus dem Dump verarbeitet und in drei Kategorien aufgeteilt: \emph{good}, \emph{promo} und \emph{neutral}. In die erste Kategorie \emph{good} fallen die wie zuvor beschrieben als lesenswert und exzellent gekennzeichneten Artikel; in die zweite Kategorie \emph{promo} die anhand der Templates als werbend erkannten und in die letzte Kategorie \emph{neutral} alle weiteren Artikel (mit den beschriebenen Ausschlüssen der Nicht-Artikel-Seiten). Die zur Kategorisierung genutzten Templates sowie alle Zeilenumbrüche wurden aus dem Artikeltext entfernt, ansonsten wurde er unverändert übernommen.

Insgesamt ergab sich die folgende Aufteilung:
\begin{itemize}
    \item \emph{good}: 46.882
    \item \emph{promo}: 32.633
    \item \emph{neutral}: 6.611.303
    \item \emph{skipped}: 17.401.113
\end{itemize}

Die kategorisierten Artikel wurden ähnlich dem Kaggle-Datensatz in CSV-Dateien geschrieben, deren Zeilen neben dem Text die Seiten-ID und den Titel enthalten sowie bei \emph{promo} noch Label zur Klassifikation des werblichen Inhalts.

Da die Kategorien extrem ungleich verteilt sind (neutrale Artikel etwa um einen Faktor 200 häufiger als werbende), wurde anschließend mittels \textit{Reservoir Sampling} \cite{ReservoirSampling} noch eine zufällige Auswahl der Artikel in weitere CSV-Dateien geschrieben, um jeweils gleich viele Samples der drei Kategorien zu erhalten. Auf dieser Auswahl konnten dann wie beim Kaggle-Datensatz die verschiedenen Lernverfahren trainiert werden.

\subsubsection{Augmentierung der Daten}
Da es nicht sicher ist, ob der Wikimedia Dump das Problem vollkommen lösen kann, wurde neben dem Hinzuziehen weiterer Daten auch versucht die Daten zu Augmentieren. Dabei sollten besonders untervetretene Klassen mehr Repräsentanten kriegen. Dabei wurden verschiedene Methoden ausprobiert, um die Daten zu augmentieren. Diese Methoden werden in \textbf{Referenz ergänzen} vorgestellt.

\subsection{Problemdefinition}
Das Ziel dieses Projekts ist die Entwicklung von Modellen zur automatisierten Klassifikation von Wikipedia-Artikeln als \emph{promotional} (werblich) oder \emph{nicht-promotional}. Dabei wird ebenfalls klassifiziert, wie ein Artikel promotional ist, also z.B. ob er eine Werbung, ein PR-Artikel usw. ist. Wikipedia strebt nach objektiven und neutralen Inhalten; daher ist die Identifizierung von Artikeln mit werbenden Charakter von großer Bedeutung, um die sachliche Qualität der Plattform zu gewährleisten.

\subsection{Zielsetzung}

Die Hauptziele des Projekts sind:

\begin{itemize} \item Entwicklung von drei klassischen maschinellen Lernmodellen und einem Deep-Learning-Modell zur Klassifikation von Wikipedia-Artikeln. \item Vergleich der Modelle anhand von Leistungsmetriken wie Genauigkeit, Präzision, Recall und F1-Score. \item Identifikation des Modells mit der besten Leistung für die gegebene Aufgabe. \end{itemize}