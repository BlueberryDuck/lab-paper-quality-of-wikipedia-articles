Wikipediaartikel sind weit studierte und verwendete Artefakte, wenn es um die Textverarbeitung geht. Sie werden zum einen als Trainingsdaten für verschiedene Modelle des maschiniellen Lernens wie z.B. BERT \cite{BERTReference} und FastText \cite{DBLP:journals/corr/BojanowskiGJM16} verwendet. Zum Anderen wurden einige Modelle entwickelt, die sich mit verschiedensten Problemstellungen bzgl. Wikipediaartikeln beschäftigen, wie z.B. Hassan et al. \cite{shavarani2020multiclassmultilingualclassificationwikipedia} und Paramita et al. \cite{das2024languageagnosticmodelingwikipediaarticles}. 
%Bis hier habe ich geschrieben

%Im folgenden Text habe ich ein paar Dinge bearbeitet
Das Ziel dieses Projektpraktikums ist die Entwicklung eines Modells, um Wikipedia-artikel zu klassifizieren. Dabei sollen Artikel auf ihre Objektivität klassifiziert werden. Um das zu erreichen wurden verschiedene Modelle des maschiniellen Lernens trainiert und evaluiert. Die Vorgehensweise zur Bearbeitung der Problemstellung entsprach dem Data Science Life Cycle. Abschnitt \ref{Aufgabenverteilung} und Abschnitt \ref{Organisation} gehen auf die organisatorischen Aspekte des Projektes ein. Abschnitt \ref{Datensatz} geht auf die Datenanalyse und die Definierung der Problemstellung ein. Darüber hinaus wird in diesem Abschnitt auch über weitere hinzugezogene Datensätze gesprochen. In Abschnitt \ref{Ansätze} werden die verwendeten Modelle vorgestellt. Die verwendeten Modelle waren der Bayes-Klassifikator \ref{sec:Bayes-Klassifikator}, die Support-Vector-Machine \ref{SVM}, die Logistische Regression \ref{Logistische Regression}, ein Convolutionales neuronales Netzwerk \ref{CNN} und ein Transformermodell \ref{Transformer}. Abschnitt \ref{Experimente} beschreibt die durchgeführten Experimente und in Abschnitt \ref{Ausblick} werden die Ergebnisse noch einmal zusammengefasst und bewertet.
%Bis hier habe ich bearbeitet.
%Darunter waren \\

%https://aclanthology.org/W09-3302.pdf