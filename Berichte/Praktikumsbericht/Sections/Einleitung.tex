Wikipedia-Artikel sind weit studierte und verwendete Artefakte, wenn es um die Textverarbeitung geht. Sie werden zum einen als Trainingsdaten für verschiedene Modelle des maschiniellen Lernens wie z.B. BERT \cite{BERTReference} und FastText \cite{DBLP:journals/corr/BojanowskiGJM16} verwendet. Zum Anderen wurden einige Modelle entwickelt, die sich mit verschiedensten Problemstellungen bzgl. Wikipediaartikeln beschäftigen, wie z.B. Hassan et al. \cite{shavarani2020multiclassmultilingualclassificationwikipedia} und Paramita et al. \cite{das2024languageagnosticmodelingwikipediaarticles}. .
%Bis hier habe ich geschrieben


%Im folgenden Text habe ich ein paar Dinge bearbeitet
Das Ziel dieses Projektpraktikums ist die Entwicklung eines Modells, um Wikipedia-artikel zu klassifizieren. Dabei sollen Artikel auf ihre Objektivität klassifiziert werden. Um das zu erreichen, wurden verschiedene Modelle des maschiniellen Lernens trainiert und evaluiert. Die Vorgehensweise zur Bearbeitung der Problemstellung entsprach dem Data Science Life Cycle. Abschnitt \ref{Aufgabenverteilung} und Abschnitt \ref{Organisation} gehen auf die organisatorischen Aspekte des Projektes ein. Abschnitt \ref{Datensatz} geht auf die Datenanalyse und die Definierung der Problemstellung ein. Darüber hinaus wird in diesem Abschnitt auch über weitere hinzugezogene Datensätze gesprochen. In Abschnitt \ref{Ansätze} werden die verwendeten Modelle vorgestellt. Die verwendeten Modelle waren der Bayes Klassifikator \ref{Bayes-Klassifikator}, die Support Vector Machine \ref{SVM}, die Logistische Regression \ref{Logistische Regression}, ein Convolutionales neuronales Netzwerk \ref{CNN} und ein Transformermodell \ref{Transformer}. Abschnitt \ref{Experimente} beschreibt die durchgeführen Experimente und in Abschnitt \ref{Ausblick} werden die Ergebnisse noch einmal zusammengefasst und bewertet.
%Bis hier habe ich bearbeitet.
 Darunter waren \\
Im Rahmen dieses Projekts bearbeiten wir folgende Teilaufgaben:

\begin{enumerate} 
\item \textbf{Analyse des Datensatzes und Identifizierung einer geeigneten Problemstellung}: Wir untersuchen den bereitgestellten Datensatz eingehend, um ein maschinelles Lernproblem zu formulieren, das mit den vorhandenen Daten gelöst werden kann. 
\item \textbf{Aufbereitung und Vorverarbeitung des Datensatzes}: Wir bereinigen und transformieren die Daten, um sie für die Modellierung vorzubereiten. 
\item \textbf{Anwendung von drei klassischen Methoden des maschinellen Lernens}: Basierend auf den Inhalten der Kapitel 2 und 3 des Kurses \glqq Einführung in Maschinelles Lernen\grqq{} implementieren wir drei klassische Algorithmen, um die identifizierte Problemstellung zu adressieren. 
\item \textbf{Anwendung eines Deep-Learning-Ansatzes}: Wir recherchieren einen geeigneten Deep-Learning-Ansatz, setzen diesen um und wenden diesen auf die Problemstellung an
\item \textbf{Entwickeln eines eigenen Ansatzes}: Im Rahmen dieser Ausarbeitung wird ein eigener Ansatz für die Problemstellung entwickelt und beschrieben. 
\item \textbf{Interpretation und Diskussion der Ergebnisse}: Basierend auf den bisherigen Resultaten entwickeln wir eine neue Idee für einen passenden Ansatz, beispielsweise eine neue Architektur für ein neuronales Netzwerk, die wir implementieren und anwenden. \end{enumerate}
%https://aclanthology.org/W09-3302.pdf