\section{Zusammenfassung und Fazit}
\label{ZusammenfassungUndFazit}
Das Ziel dieser Arbeit war die Untersuchung von Modellen zur Analyse der Qualität von Wikipediaartikeln. Dabei wurde zunächst der gegebenen Datensatz analysiert und Problemstellungen erarbeitet. Dieser Datensatz wrude mithilfe des Wikipedia Dumps erweitert. Weitere Versuche der Datenaugmentierung haben stattgefunden. Es wurden im Rahmen des Projekts fünf Modelle implementiert und ausgewertet. Anschliessend wurden die Ergebnisse verglichen und ausgewertet. Artefakte, welche durch die Arbeit entstanden, waren eine Pipeline, welche die 5 Modelle implementiert und eine flexible Verwendung der drei klassischen Ansätze erlaubt, wobei verschiedene Vorverarbeitungsmethoden und Vektorisierungen verwendet werden können. Ausserdem entstand durch das Praktikum ein riesiger Wikipedia Dump Datensatz, welcher 56 GB Daten zur Verfügung stellt.
Der Schluss aus der Auswertung der Resultate war, dass die Binäre und Dreiklassenklassifikation sehr gute Resultate erzielt haben. Dabei schnitten SVM und logistische Regression mit 96 Prozent Recall bei der binären Klassifikation und mit 90 Prozent bei der Dreiklassenklassifikation am Besten ab. Die logistische Regression ist das sinnvollste Modell, weil es schneller als das SVM trainiert wird. Bei der Multilabelklassifizierung wurden weniger gute Resultate erzielt. Dort schnitt der Bayes Klassifikator mit 66 Prozentiger Sensitivtät am Besten ab. Um die Forschung fortzusetzen, könnte man weitere Modelle implementieren. Dabei könnte man z.B. SetFit verwenden, welches zur Bekämpfung der unterrepräsentierten Klassen helfen kann. Dieses Problem könnte auch durch das Hinzuziehen weiterer Daten abgeschwächt werden. Ausserdem könnte man die Forschung auf andere Artikel als nur Wikipediaartikel ausweiten. Dabei könnte man die trainierten Modelle auf z.B. wissenschaftlichen Artikeln evaluieren. 