\section{Zusammenfassung und Fazit}
\label{ZusammenfassungUndFazit}
Das Ziel dieser Arbeit war die Untersuchung von Modellen zur Analyse der Qualität von Wikipediaartikeln. Dabei wurde zunächst der gegebene Datensatz analysiert und Problemstellungen erarbeitet. Dieser Datensatz wrude mithilfe des Wikipedia Dumps erweitert. Weitere Versuche der Datenaugmentierung haben stattgefunden. Es wurden im Rahmen des Projekts fünf Modelle implementiert und ausgewertet. Anschliessend wurden die Ergebnisse verglichen und evaluiert. Artefakte, welche durch die Arbeit entstanden, waren eine Pipeline, welche die 3 klassischen Modelle implementiert und eine flexible Verwendung erlaubt, wobei verschiedene Vorverarbeitungsmethoden und Vektorisierungen verwendet werden können. Getrennt davon wurden die Deep Learning Ansätze erarbeitet und können z.T. auch flexibel verwendet werden. Ausserdem entstand durch das Praktikum ein riesiger Wikipedia Dump Datensatz, welcher 56 GB Daten zur Verfügung stellt.
Der Schluss aus der Auswertung der Resultate war, dass die Binäre und Dreiklassenklassifikation sehr gute Resultate erzielt haben. Dabei schnitten SVM, KNN und logistische Regression mit 96 Prozent Recall bei der binären Klassifikation und SVM und logistische Regression mit 90 Prozent bei der Dreiklassenklassifikation am Besten ab. Die logistische Regression ist das sinnvollste Modell, weil es schneller als das SVM trainiert wird. Bei der Multilabelklassifizierung wurden weniger gute Resultate erzielt. Dort schnitt der Bayes Klassifikator mit 66 Prozentiger Sensitivtät am Besten ab.