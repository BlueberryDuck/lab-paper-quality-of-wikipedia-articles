\begin{itemize}
    \item Erstellung eines Konverters, um den Dump der englischsprachigen Wikipedia umzuwandeln in ein Format, dass mit dem Kaggle-Datensatz kompatibel ist (siehe \ref{WPDump}).
    \item Explorative Datenanalyse mit Jupyter Notebooks.
    \item Implementierung des SVM-Klassifikators in zwei Varianten, beschrieben in \ref{SVM}. Zunächst experimentell auf eigenem Branch, später Integration in die zentrale Pipeline.
    \item Option zum Verfälschen einer zufälligen Teilmenge des Datensatzes für Experimente. Die Größe der Teilmenge ist als Bruchteil der Gesamtdaten konfigurierbar.
    \item Kleinere Anpassungen an der Pipeline um den Wikipedia-Dump für das Training und die Evaluation der Modelle heranziehen zu können.
    \item Folien für Zwischenpräsentation und finale Präsentation.
    \item Teile des Abschlussberichts zur Beschreibung des Wikipedia-Dump-Konverters und der SVM.
\end{itemize}
