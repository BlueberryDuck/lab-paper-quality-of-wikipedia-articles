



@misc{hendrycks2023gaussianerrorlinearunits,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}

@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}



@misc{tunstall2022efficientfewshotlearningprompts,
      title={Efficient Few-Shot Learning Without Prompts}, 
      author={Lewis Tunstall and Nils Reimers and Unso Eun Seo Jo and Luke Bates and Daniel Korat and Moshe Wasserblat and Oren Pereg},
      year={2022},
      eprint={2209.11055},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.11055}, 
}



%Ab hier ist alt.




@inproceedings{Ansel2024,
  author     = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  booktitle  = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  title      = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  year       = {2024},
  month      = apr,
  pages      = {929--947},
  publisher  = {ACM},
  series     = {ASPLOS ’24},
  collection = {ASPLOS ’24},
  doi        = {10.1145/3620665.3640366}
}


@book{Bishop2019,
  author    = {Bishop, Christopher M.},
  publisher = {Springer Science+Business Media, LLC},
  title     = {Pattern recognition and machine learning},
  year      = {2019},
  address   = {New York, NY},
  series    = {Information Science and Statistics},
  pagetotal = {758},
  ppn_gvk   = {1777667275}
}


@article{Bojanowski2016,
  author        = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  title         = {Enriching Word Vectors with Subword Information},
  year          = {2016},
  month         = jul,
  abstract      = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1607.04606},
  eprint        = {1607.04606},
  file          = {:Bojanowski2016 - Enriching Word Vectors with Subword Information.pdf:PDF:http\://arxiv.org/pdf/1607.04606v2},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Das2024,
  author        = {Das, Paramita and Johnson, Isaac and Saez-Trumper, Diego and Aragón, Pablo},
  title         = {Language-Agnostic Modeling of Wikipedia Articles for Content Quality Assessment across Languages},
  year          = {2024},
  month         = apr,
  abstract      = {Wikipedia is the largest web repository of free knowledge. Volunteer editors devote time and effort to creating and expanding articles in more than 300 language editions. As content quality varies from article to article, editors also spend substantial time rating articles with specific criteria. However, keeping these assessments complete and up-to-date is largely impossible given the ever-changing nature of Wikipedia. To overcome this limitation, we propose a novel computational framework for modeling the quality of Wikipedia articles. State-of-the-art approaches to model Wikipedia article quality have leveraged machine learning techniques with language-specific features. In contrast, our framework is based on language-agnostic structural features extracted from the articles, a set of universal weights, and a language version-specific normalization criterion. Therefore, we ensure that all language editions of Wikipedia can benefit from our framework, even those that do not have their own quality assessment scheme. Using this framework, we have built datasets with the feature values and quality scores of all revisions of all articles in the existing language versions of Wikipedia. We provide a descriptive analysis of these resources and a benchmark of our framework. In addition, we discuss possible downstream tasks to be addressed with these datasets, which are released for public use.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2404.09764},
  eprint        = {2404.09764},
  file          = {:Das2024 - Language Agnostic Modeling of Wikipedia Articles for Content Quality Assessment across Languages.pdf:PDF:http\://arxiv.org/pdf/2404.09764v1},
  keywords      = {Computers and Society (cs.CY), FOS: Computer and information sciences},
  primaryclass  = {cs.CY},
  publisher     = {arXiv}
}

@article{Devlin2018,
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year          = {2018},
  month         = oct,
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1810.04805},
  eprint        = {1810.04805},
  file          = {:Devlin2018 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF:http\://arxiv.org/pdf/1810.04805v2},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Hendrycks2016,
  author        = {Hendrycks, Dan and Gimpel, Kevin},
  title         = {Gaussian Error Linear Units (GELUs)},
  year          = {2016},
  month         = jun,
  abstract      = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1606.08415},
  eprint        = {1606.08415},
  file          = {:Hendrycks2016 - Gaussian Error Linear Units (GELUs).pdf:PDF:http\://arxiv.org/pdf/1606.08415v5},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv}
}

@inbook{Joachims1998,
  author    = {Joachims, Thorsten},
  pages     = {137--142},
  publisher = {Springer Berlin Heidelberg},
  title     = {Text categorization with Support Vector Machines: Learning with many relevant features},
  year      = {1998},
  isbn      = {9783540697817},
  booktitle = {Machine Learning: ECML-98},
  doi       = {10.1007/bfb0026683},
  issn      = {1611-3349}
}

@book{Manning2009,
  author    = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  publisher = {Cambridge Univ. Press},
  title     = {Introduction to information retrieval},
  year      = {2009},
  address   = {Cambridge [u.a.]},
  edition   = {Reprinted},
  isbn      = {9780521865715},
  note      = {Includes bibliographical references and index},
  pagetotal = {482},
  ppn_gvk   = {614978130}
}

@misc{Mikolov2013,
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  year      = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1301.3781},
  eprint    = {1301.3781},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv}
}

@article{Pedregosa2011,
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  title   = {Scikit-learn: Machine Learning in {P}ython},
  year    = {2011},
  pages   = {2825--2830},
  volume  = {12}
}

@inproceedings{Pennington2014,
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {Glove: Global Vectors for Word Representation},
  year      = {2014},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/d14-1162}
}

@article{Porter2006,
  author    = {Porter, M.F.},
  journal   = {Program},
  title     = {An algorithm for suffix stripping},
  year      = {2006},
  issn      = {0033-0337},
  month     = jul,
  number    = {3},
  pages     = {211--218},
  volume    = {40},
  doi       = {10.1108/00330330610681286},
  publisher = {Emerald}
}

@article{Sanh2019,
  author        = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  year          = {2019},
  month         = oct,
  abstract      = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1910.01108},
  eprint        = {1910.01108},
  file          = {:Sanh2019 - DistilBERT, a Distilled Version of BERT_ Smaller, Faster, Cheaper and Lighter.pdf:PDF:http\://arxiv.org/pdf/1910.01108v4},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Shavarani2019,
  author        = {Shavarani, Hassan S. and Sekine, Satoshi},
  title         = {Multi-class Multilingual Classification of Wikipedia Articles Using Extended Named Entity Tag Set},
  year          = {2019},
  month         = sep,
  abstract      = {Wikipedia is a great source of general world knowledge which can guide NLP models better understand their motivation to make predictions. Structuring Wikipedia is the initial step towards this goal which can facilitate fine-grain classification of articles. In this work, we introduce the Shinra 5-Language Categorization Dataset (SHINRA-5LDS), a large multi-lingual and multi-labeled set of annotated Wikipedia articles in Japanese, English, French, German, and Farsi using Extended Named Entity (ENE) tag set. We evaluate the dataset using the best models provided for ENE label set classification and show that the currently available classification models struggle with large datasets using fine-grained tag sets.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1909.06502},
  eprint        = {1909.06502},
  file          = {:Shavarani2019 - Multi Class Multilingual Classification of Wikipedia Articles Using Extended Named Entity Tag Set.pdf:PDF:http\://arxiv.org/pdf/1909.06502v2},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Tunstall2022,
  author        = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  title         = {Efficient Few-Shot Learning Without Prompts},
  year          = {2022},
  month         = sep,
  abstract      = {Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2209.11055},
  eprint        = {2209.11055},
  file          = {:Tunstall2022 - Efficient Few Shot Learning without Prompts.pdf:PDF:http\://arxiv.org/pdf/2209.11055v1},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@misc{Urbanbricks2020,
  author       = {Urbanbricks},
  howpublished = {\url{https://www.kaggle.com/datasets/urbanbricks/wikipedia-promotional-articles}},
  note         = {Kaggle-Datensatz, abgerufen am 04. Oktober 2024},
  title        = {Wikipedia Promotional Articles},
  year         = {2020},
  publisher    = {Kaggle}
}

@article{Vaswani2017,
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title         = {Attention Is All You Need},
  year          = {2017},
  month         = jun,
  abstract      = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1706.03762},
  eprint        = {1706.03762},
  file          = {:Vaswani2017 - Attention Is All You Need.pdf:PDF:http\://arxiv.org/pdf/1706.03762v7},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Vitter1985,
  author    = {Vitter, Jeffrey S.},
  journal   = {ACM Transactions on Mathematical Software},
  title     = {Random sampling with a reservoir},
  year      = {1985},
  issn      = {1557-7295},
  month     = mar,
  number    = {1},
  pages     = {37--57},
  volume    = {11},
  doi       = {10.1145/3147.3165},
  publisher = {Association for Computing Machinery (ACM)}
}

@article{Wei2019,
  author        = {Wei, Jason and Zou, Kai},
  title         = {EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks},
  year          = {2019},
  month         = jan,
  abstract      = {We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1901.11196},
  eprint        = {1901.11196},
  file          = {:Wei2019 - EDA_ Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.pdf:PDF:http\://arxiv.org/pdf/1901.11196v2},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}
@misc{Yang2019,
  author = {Ziyi Yang and Chenguang Zhu and Weizhu Chen},
  title  = {Zero-training Sentence Embedding via Orthogonal Basis},
  year   = {2019},
  url    = {https://openreview.net/forum?id=rJedbn0ctQ}
}
