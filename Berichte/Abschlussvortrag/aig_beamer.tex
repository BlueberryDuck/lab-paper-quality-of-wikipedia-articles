%%% Choose between 16:9 and 4:3 format by commenting out/uncommenting one of the following lines:
\documentclass[aspectratio=169]{beamer} % 16:9
% \documentclass{beamer} % 4:3

%=========================================================================================================================

\usepackage[english]{babel}     % English language
\usepackage[utf8]{inputenc}     % Input encoding changed to utf8 for proper rendering of umlauts
\usepackage{tikz}               % For creating graphics
\usepackage{subfig}
\usepackage[mode=buildnew]{standalone}
\usepackage{url}                % For including urls
\usepackage{tabularx}           % For better tables
\usepackage{xcolor}             % Für bunte Farbe :)

\usetheme{aig}                  % Set beamer theme

%=========================================================================================================================
\title{Qualität von Wikipedia-Artikeln}
\author[Kunze, Bunge, Krämer, Steenhof, Suxdorf]{Alexander Kunze, Sebastian Bunge, Johannes Krämer, Emmanuelle Steenhof, Robin Suxdorf}
\institute{Artificial Intelligence Group,\\
University of Hagen, Germany}
\date{\today}
%=========================================================================================================================
\logo{\includegraphics[width=3cm]{figures/logoaig.png}}
%=========================================================================================================================

\begin{document}

%=========================================================================================================================

% frame
% block
% alertblock
% exampleblock

% \highlight{}
% \darkhighlight{}
% \yellowhighlight{}
% \mathhighlight{}
% \darkmathhighlight{}
% \yellowmathhighlight{}

% appendix

\begin{frame}
    \titlepage
\end{frame}
\nologo

\begin{frame}{Motivation}
    (Screenshot guter Artikel)
\end{frame}

\begin{frame}{Motivation}
    (Screenshot schlechter Artikel)
\end{frame}

\section{Daten}

\begin{frame}{Daten}
    % Es sollte etwas zur Einleitung der Section gesagt werden
\end{frame}

\subsection{Datensatz}

\begin{frame}{Datensatz}
    (Verteilung good/promo)
    (Verteilung labels)
\end{frame}

\subsection{Problemstellung}

\begin{frame}{Problemstellung}

\end{frame}

\subsection{Weitere Daten}

\begin{frame}{Weitere Daten}

\end{frame}

\subsection{Vorverarbeitung}

\begin{frame}{Vorverarbeitung}
    \begin{block}{Datensatzvorverarbeitung}
        \begin{itemize}
            \item Extrahieren von Labeln aus Metadaten
            \item Reduzieren auf Artikeltext und Label
            \item Zusammenführen mehrerer Dateien in ein DataFrame
        \end{itemize}
    \end{block}
    \begin{block}{Textvorverarbeitung}
        \begin{itemize}
            \item Entfernen von Sonder- und Interpunktionszeichen
            \item Umwandeln in Kleinbuchstaben
            \item Entfernen von Stoppwörtern
            \item Umwandeln mit Stemming (PorterStemmer)
            \item Zahlen bleiben erhalten
        \end{itemize}
    \end{block}
\end{frame}

% Kandidat für Appendix
\begin{frame}{Vorverarbeitung}
    \begin{exampleblock}{Textvorverarbeitung Artikel 39582}
        \begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
            \textbf{Original} & \textbf{Verarbeitete Version}                                                                                                                                                                                                 \\ \hline
            The Human Research Program HRP was created in October 2005 at Johnson Space Center JSC in response to NASA's desire to move human research project management away from headquarters to JSC and to focus its research investment on investigating and mitigating the highest risks to astronaut health and performance in support of exploration missions...
                              &
            human research program hrp creat octob 2005 johnson space center jsc respons nasa desir move human research project manag away headquart jsc focu research invest investig mitig highest risk astronaut health perform support explor mission ... \\
        \end{tabularx}
    \end{exampleblock}
\end{frame}

\begin{frame}{Vorverarbeitung}
    \begin{block}{TF-IDF Vektorisierung}
        \begin{itemize}
            \item Term Frequency-Inverse Document Frequency (TF-IDF) zur Textvektorisierung
            \item Bewertet Wichtigkeit eines Wortes in einem Dokument relativ zum Korpus
            \item Vorteile:
                  \begin{itemize}
                      \item Reduziert Einfluss häufiger, aber uninformativer Wörter
                      \item Hebt diskriminierende Begriffe hervor
                      \item Erzeugt spärliche Merkmalsvektormatrizen
                  \end{itemize}
            \item Parameter:
                  \begin{itemize}
                      \item \texttt{ngram\_range: [1, 1]} - Nur Einzelwörter werden betrachtet
                            % Mehrwort-Ausdrücke (n-grams) werden nicht berücksichtigt
                      \item \texttt{max\_df: 0.9} - Ignoriert Korpus-spezifische Stopwörter ($>90\%$)
                            % Entfernt häufige Wörter, die in fast allen Dokumenten vorkommen
                      \item \texttt{min\_df: 0.001} - Filtert Rauschen und Rechtschreibfehler ($<0.1\%$)
                            % Entfernt sehr seltene Wörter, reduziert Vokabulargröße und Overfit-Risiko
                      \item \texttt{max\_features: 10\_000} - Beschränkt die Vokabulargröße auf 10.000
                            % Verbessert Laufzeiteffizienz und verringert Overfit-Risiko
                      \item \texttt{sublinear\_tf: true} - Logarithmische TF-Skalierung für lange Dokumente
                            % Reduziert Verzerrungen durch mehrfach vorkommende Wörter in einem Dokument
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Easy Data Augmentation (EDA)}
    \begin{block}{Ziel und Methode}
        \begin{itemize}
            \item Idee: Die Anwendung von Easy Data Augmentation (EDA), um durch augmentierte Daten die Anzahl der unterrepräsentierten Labels zu vergrößern.
        \end{itemize}
    \end{block}

    \begin{block}{Umsetzung}
        \begin{itemize}
            \item Es wurden neue Texte aus den Originaltexten erstellt indem Synonyme hinzugefügt, zufällig Wortreihenfolgen getauscht und zufällig Wörter eingefügt bzw. gelöscht worden sind.
        \end{itemize}
    \end{block}

    \begin{alertblock}{Ergebnisse}
        \begin{itemize}
            \item Auf dem augmentierten Datensatz trainierte Modelle verbesserten sich signifikant, wenn auf den augmentierten Daten getestet worden ist. Das augmentierte Modell schnitt aber deutlich schlechter ab, wenn auf dem Originalsatz getestet worden ist. Damit zeigt sich der Verdacht von Overfitting. $\to$ \textbf{Ansatz verworfen}.
        \end{itemize}
    \end{alertblock}
\end{frame}

\section{Ansätze}

\begin{frame}{Ansätze}
    \begin{block}{Klassische Verfahren}
        \begin{itemize}
            \item Logistische Regression
            \item Bayes-Klassifikator
            \item Support Vector Machine
        \end{itemize}
    \end{block}
    \begin{block}{Deep Learning}
        \begin{itemize}
            \item \yellowhighlight{CNN}
            \item \yellowhighlight{5. Ansatz}
        \end{itemize}
    \end{block}
\end{frame}

\subsection{Klassische Verfahren}

\begin{frame}{Klassische Verfahren}
    \begin{block}{Logistische Regression}
        \begin{itemize}
            \item Diskriminativer Klassifikator, der Entscheidungsgrenzen direkt modelliert
            \item Schätzt die Wahrscheinlichkeiten von Klassen basierend auf einer linearen Entscheidungsfunktion
            \item Geeignet für Textklassifikation mit hochdimensionalen Feature-Vektoren
            \item Vorteile:
                  \begin{itemize}
                      \item Lineare Zeitkomplexität O(nd) % SB: Sicher? Logreg hatte mit Abstand das längste Training der drei klassischen Modelle
                      \item Einfach Implementation
                      \item Schnell in der Berechnung.
                  \end{itemize}
            \item Parameter:
                  \begin{itemize}
                      \item \texttt{Regularisierung}: L1-Regularisierung
                      \item \texttt{OneVsRestClassifier}: Für die Klassifizierung der Multilabeldaten
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Klassische Verfahren}
    \begin{block}{Multinomialer Naive Bayes-Klassifikator}
        \begin{itemize}
            \item Probabilistischer Klassifikator, der auf Bayes'schem Theorem basiert
            \item Nimmt bedingte Unabhängigkeit zwischen Features an (naive Annahme)
            \item Geeignet für Textklassifikation mit Wortvorkommen als Features
            \item Vorteile:
                  \begin{itemize}
                      \item Lineare Zeitkomplexität O(nd) % n = Anzahl der Features, d = Anzahl der Klassen
                            % Mit Abstand der schnelleste Klassifikator gerade beim gesampelten Wikipedia-Dump
                            % Training auf den samples war nach Sekunden abgeschlossen
                      \item Robust bei hoher Dimensionalität
                            % Die vektorisierten Features sind hochdimensional
                            % (Definition hochdimensional?)
                      \item Native Unterstützung für spärliche Matrizen
                            % die vektorisierten Features sind spärliche Matrizen
                  \end{itemize}
            \item Parameter:
                  \begin{itemize}
                      \item \texttt{alpha}: Laplace-Glättung
                            % Wird verwendet, um Überanpassung zu vermeiden, indem Null-Wahrscheinlichkeiten durch eine kleine positive Konstante ersetzt werden
                      \item \texttt{fit\_prior}: Lernen der Klassenpriors
                            % Wird verwendet, um die Klassenverteilung aus den Trainingsdaten zu schätzen bevor man irgendwelche Features sieht. Wenn False wird gleiche Klassenwahrscheinlichkeit angenommen
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Klassische Verfahren}
    Support Vector Machine
\end{frame}

\subsection{Deep Learning}

\begin{frame}{Deep Learning}
    CNN
\end{frame}

\begin{frame}{Deep Learning}
    \begin{block}{5. Ansatz}
        Probleme: Schlechte Multilabelklassifizierung der ersten vier Ansätze. \\
        Analyse: Unterscheidung der Promotional aufgrund von Thema: \\
        Ziel:
        Einbezug von Kontext/Bedeutung \\
        Ideen:
        \begin{itemize}
            \item Vorverarbeitungsmethode erstellen
            \item Transformerarchitektur verwenden
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Deep Learning}
    \begin{block}{Transformerarchitektur}
        Modellwahl: DistilBERT \\
        Vorteile: Besondere Vorteile von BERT in der Erkennung von Kontext \\
        Architektur:
        \begin{itemize}
            \item Darstellung: DistilBERT Tokenizer
            \item Attention: Scaled Dot Product Attention (SDPA)
            \item Aktivierungsfunktion: $GeLu(x) \approx {0.5 \cdot x \cdot (1+ tanh({\frac{x}{\sqrt{2}}}))} $
        \end{itemize}
    \end{block}
\end{frame}

\section{Zusammenfassung}

\begin{frame}{Zusammenfassung}
    \begin{block}{Zusammenfassung}
        Errungenschaften:
        \begin{itemize}
            \item Daten analysiert
            \item Datensatz mit Wikipedia Dump erweitert
            \item Problemstellung erfasst
            \item Datensatz für Mashine Learning Verfahren vorbereitet.
            \item Drei Klassische Modelle implementiert
            \item Zwei Neuronale Methode implementiert
            \item Ergebnisse Evaluiert
        \end{itemize}
        Ergebnisse:
        \begin{itemize}
            \item Datensatz aus Wikipedia Dump erstellt
            \item Pipeline zur Klassifizierung von Wikipediaartikeln implementiert
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Zusammenfassung}
    \begin{block}{Zusammenfassung}
        Misserfolge
        \begin{itemize}
            \item Datenaugmentierung
            \item Entwicklung von Vorverarbeitung zur Erkennung von Kontext
        \end{itemize}
        Erweiterungsmöglichkeiten:
        \begin{itemize}
            \item Weitere klassische und neuronalen Ansätze implementieren und evaluieren
            \item Ganzen Wikipedia Dump verwenden
            \item Verwendung von SetFit
            \item Verwendung weiterer Modelle des Maschinellen Lernens
        \end{itemize}
    \end{block}
\end{frame}

\appendix
\input{appendix.tex}

\end{document}
